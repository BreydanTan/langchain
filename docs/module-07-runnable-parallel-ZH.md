# æ¨¡å— 7ï¼š`RunnableParallel` - å¹¶è¡Œæ‰§è¡Œæ·±åº¦å‰–æ

> **å­¦ä¹ ç›®æ ‡**
> - ç†è§£ `RunnableParallel` çš„è®¾è®¡æ€æƒ³å’Œä½¿ç”¨åœºæ™¯
> - æŒæ¡å¹¶è¡Œæ‰§è¡Œçš„å†…éƒ¨å®ç°æœºåˆ¶ï¼ˆçº¿ç¨‹æ±  vs å¼‚æ­¥ï¼‰
> - å­¦ä¹ å¦‚ä½•é€šè¿‡å¹¶è¡Œæå‡æ€§èƒ½å’Œæ„å»ºå¤æ‚æ•°æ®æµ

---

## ğŸ“ æœ¬æ¨¡å—åœ¨æ•´ä½“æ¶æ„ä¸­çš„ä½ç½®

```mermaid
graph TD
    A[Runnable æ ¸å¿ƒæŠ½è±¡] --> B[ç»„åˆåŸè¯­]
    B --> C[RunnableSequence<br>é¡ºåºæ‰§è¡Œ]
    B --> D[RunnableParallel<br>å¹¶è¡Œæ‰§è¡Œ]
    B --> E[RunnableLambda<br>è‡ªå®šä¹‰å‡½æ•°]

    style D fill:#ff6b6b,color:#fff
```

**åœ°ä½è¯´æ˜ï¼š** `RunnableParallel` æ˜¯ä¸ `RunnableSequence` å¹¶åˆ—çš„**ä¸¤å¤§æ ¸å¿ƒç»„åˆåŸè¯­ä¹‹ä¸€**ï¼Œä¸“é—¨ç”¨äºå¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†æ”¯ã€‚

---

## ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦ `RunnableParallel`ï¼Ÿ

### é—®é¢˜ 1ï¼šå¤šä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ

å‡è®¾ä½ éœ€è¦åŒæ—¶ï¼š
- ç”Ÿæˆæ–‡ç« æ‘˜è¦
- ç¿»è¯‘æ–‡ç« åˆ°æ³•è¯­
- æå–å…³é”®è¯

**ä¸²è¡Œæ‰§è¡Œï¼ˆæ…¢ï¼‰ï¼š**
```python
summary = summarize_chain.invoke(text)      # ç­‰å¾… 2 ç§’
translation = translate_chain.invoke(text)   # ç­‰å¾… 2 ç§’
keywords = extract_keywords.invoke(text)     # ç­‰å¾… 1 ç§’
# æ€»è€—æ—¶ï¼š5 ç§’
```

**å¹¶è¡Œæ‰§è¡Œï¼ˆå¿«ï¼‰ï¼š**
```python
parallel = RunnableParallel(
    summary=summarize_chain,
    translation=translate_chain,
    keywords=extract_keywords,
)
result = parallel.invoke(text)
# æ€»è€—æ—¶ï¼šmax(2, 2, 1) = 2 ç§’
```

### é—®é¢˜ 2ï¼šæ„å»ºå¤æ‚æ•°æ®æµ

RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ¨¡å¼éœ€è¦ï¼š
1. ä¿ç•™åŸå§‹é—®é¢˜
2. åŒæ—¶æ£€ç´¢ç›¸å…³æ–‡æ¡£

```python
# ä½¿ç”¨ RunnableParallel ä¼˜é›…å®ç°
rag_chain = (
    {
        "context": retriever,              # åˆ†æ”¯ 1ï¼šæ£€ç´¢æ–‡æ¡£
        "question": RunnablePassthrough()  # åˆ†æ”¯ 2ï¼šä¿ç•™åŸå§‹é—®é¢˜
    }
    | prompt
    | model
)
```

---

## ğŸ—ï¸ æºç æ·±åº¦è§£æ

### ğŸ“„ æ–‡ä»¶ä½ç½®
**`libs/core/langchain_core/runnables/base.py:3537-3936`**

### 1ï¸âƒ£ ç±»å®šä¹‰ä¸æ ¸å¿ƒå±æ€§

```python
class RunnableParallel(RunnableSerializable[Input, dict[str, Any]]):
    """
    å¹¶è¡Œè¿è¡Œå¤šä¸ª Runnableï¼Œæ‰€æœ‰åˆ†æ”¯æ¥æ”¶**ç›¸åŒçš„è¾“å…¥**ã€‚
    è¿”å›å€¼ï¼šå­—å…¸ï¼Œé”®ä¸ºåˆ†æ”¯åï¼Œå€¼ä¸ºå„åˆ†æ”¯çš„è¾“å‡ºã€‚
    """

    steps__: Mapping[str, Runnable[Input, Any]]
    # å­˜å‚¨æ‰€æœ‰å¹¶è¡Œåˆ†æ”¯ï¼Œé”®ä¸ºåˆ†æ”¯åï¼Œå€¼ä¸º Runnable å¯¹è±¡
```

**å…³é”®ç‚¹ï¼š**
- è¾“å…¥ç±»å‹ï¼š`Input`ï¼ˆæ‰€æœ‰åˆ†æ”¯å…±äº«ï¼‰
- è¾“å‡ºç±»å‹ï¼š`dict[str, Any]`ï¼ˆå­—å…¸å½¢å¼ï¼‰
- åˆ†æ”¯å­˜å‚¨ï¼š`steps__` å­—å…¸

---

### 2ï¸âƒ£ åˆå§‹åŒ–ï¼šä¸‰ç§åˆ›å»ºæ–¹å¼

#### æ–¹å¼ 1ï¼šä½¿ç”¨å­—å…¸è¯­æ³•ï¼ˆæ¨èï¼‰

```python
parallel = {
    "add": RunnableLambda(lambda x: x + 10),
    "multiply": RunnableLambda(lambda x: x * 2),
}
```

**æºç å®ç°ï¼š** `libs/core/langchain_core/runnables/base.py:3623-3647`

```python
def __init__(
    self,
    steps__: Mapping[str, Runnable[Input, Any] | Callable] | None = None,
    **kwargs: Runnable[Input, Any] | Callable,
) -> None:
    # åˆå¹¶ steps__ å’Œ kwargs
    merged = {**steps__} if steps__ is not None else {}
    merged.update(kwargs)

    # å°†æ‰€æœ‰å€¼å¼ºåˆ¶è½¬æ¢ä¸º Runnable
    super().__init__(
        steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
    )
```

**å…³é”®è®¾è®¡ï¼š**
1. **`coerce_to_runnable()`**ï¼šè‡ªåŠ¨å°†æ™®é€šå‡½æ•°è½¬æ¢ä¸º `RunnableLambda`
2. **çµæ´»æ€§**ï¼šæ”¯æŒ `steps__` å‚æ•° + `**kwargs` ä¸¤ç§ä¼ å‚æ–¹å¼

#### æ–¹å¼ 2ï¼šæ˜¾å¼æ„é€ 

```python
parallel = RunnableParallel(
    {"branch1": runnable1, "branch2": runnable2}
)
```

#### æ–¹å¼ 3ï¼šå…³é”®å­—å‚æ•°

```python
parallel = RunnableParallel(
    branch1=runnable1,
    branch2=runnable2
)
```

---

### 3ï¸âƒ£ æ ¸å¿ƒæ–¹æ³•ï¼š`invoke()` - å¹¶è¡Œæ‰§è¡Œ

**æºç ä½ç½®ï¼š** `libs/core/langchain_core/runnables/base.py:3806-3863`

```python
def invoke(
    self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
) -> dict[str, Any]:
    # 1. è®¾ç½®å›è°ƒç®¡ç†å™¨
    config = ensure_config(config)
    callback_manager = CallbackManager.configure(...)
    run_manager = callback_manager.on_chain_start(...)

    # 2. å®šä¹‰å•æ­¥æ‰§è¡Œå‡½æ•°
    def _invoke_step(step: Runnable, input_: Input, config: RunnableConfig, key: str):
        child_config = patch_config(
            config,
            callbacks=run_manager.get_child(f"map:key:{key}"),  # å­å›è°ƒ
        )
        return step.invoke(input_, child_config)

    try:
        # 3. å¤åˆ¶ steps é¿å…å¹¶å‘ä¿®æ”¹é—®é¢˜
        steps = dict(self.steps__)

        # 4. ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        with get_executor_for_config(config) as executor:
            futures = [
                executor.submit(_invoke_step, step, input, config, key)
                for key, step in steps.items()
            ]

            # 5. æ”¶é›†ç»“æœ
            output = {
                key: future.result()
                for key, future in zip(steps, futures, strict=False)
            }

    except BaseException as e:
        run_manager.on_chain_error(e)
        raise
    else:
        run_manager.on_chain_end(output)
        return output
```

**æ‰§è¡Œæµç¨‹å›¾ï¼š**

```mermaid
sequenceDiagram
    participant User
    participant Parallel
    participant Executor as çº¿ç¨‹æ± 
    participant Branch1
    participant Branch2

    User->>Parallel: invoke(input)
    Parallel->>Executor: åˆ›å»ºçº¿ç¨‹æ± 

    par å¹¶è¡Œæ‰§è¡Œ
        Executor->>Branch1: submit(_invoke_step)
        Executor->>Branch2: submit(_invoke_step)
    end

    Branch1-->>Executor: result1
    Branch2-->>Executor: result2

    Executor->>Parallel: {"branch1": result1, "branch2": result2}
    Parallel->>User: è¿”å›å­—å…¸ç»“æœ
```

**å…³é”®è®¾è®¡ç‚¹ï¼š**

| è®¾è®¡ç‚¹ | å®ç°æ–¹å¼ | ç›®çš„ |
|--------|----------|------|
| **çº¿ç¨‹æ± æ‰§è¡Œ** | `get_executor_for_config(config)` | CPU å¯†é›†å‹ä»»åŠ¡å¹¶è¡Œ |
| **å­å›è°ƒ** | `run_manager.get_child(f"map:key:{key}")` | è¿½è¸ªæ¯ä¸ªåˆ†æ”¯çš„æ‰§è¡Œ |
| **æµ…æ‹·è´** | `steps = dict(self.steps__)` | é˜²æ­¢å¹¶å‘ä¿®æ”¹ |
| **å¼‚å¸¸å¤„ç†** | `try-except-else` | ç¡®ä¿å›è°ƒæ­£ç¡®è§¦å‘ |

---

### 4ï¸âƒ£ å¼‚æ­¥æ–¹æ³•ï¼š`ainvoke()` - çœŸæ­£çš„å¹¶å‘

**æºç ä½ç½®ï¼š** `libs/core/langchain_core/runnables/base.py:3866-3918`

```python
async def ainvoke(
    self,
    input: Input,
    config: RunnableConfig | None = None,
    **kwargs: Any,
) -> dict[str, Any]:
    # è®¾ç½®å¼‚æ­¥å›è°ƒ
    config = ensure_config(config)
    callback_manager = get_async_callback_manager_for_config(config)
    run_manager = await callback_manager.on_chain_start(...)

    async def _ainvoke_step(step, input_, config, key):
        child_config = patch_config(config, callbacks=run_manager.get_child(f"map:key:{key}"))
        return await coro_with_context(
            step.ainvoke(input_, child_config),
            context,
            create_task=True
        )

    try:
        steps = dict(self.steps__)

        # ä½¿ç”¨ asyncio.gather çœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(
            *(_ainvoke_step(step, input, config, key) for key, step in steps.items())
        )

        output = dict(zip(steps, results, strict=False))
    except BaseException as e:
        await run_manager.on_chain_error(e)
        raise
    else:
        await run_manager.on_chain_end(output)
        return output
```

**`invoke()` vs `ainvoke()` å¯¹æ¯”ï¼š**

| ç‰¹æ€§ | `invoke()` | `ainvoke()` |
|------|------------|-------------|
| **å¹¶è¡Œæ–¹å¼** | çº¿ç¨‹æ± ï¼ˆ`ThreadPoolExecutor`ï¼‰ | å¼‚æ­¥åç¨‹ï¼ˆ`asyncio.gather`ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | I/O å¯†é›†å‹ï¼ˆç½‘ç»œè¯·æ±‚ã€API è°ƒç”¨ï¼‰ | å¼‚æ­¥ I/Oï¼ˆasync HTTPã€async DBï¼‰ |
| **å¹¶å‘ç²’åº¦** | è¿›ç¨‹çº§çº¿ç¨‹ | äº‹ä»¶å¾ªç¯åç¨‹ |
| **å¼€é”€** | è¾ƒé«˜ï¼ˆçº¿ç¨‹åˆ‡æ¢ï¼‰ | ä½ï¼ˆåç¨‹åˆ‡æ¢ï¼‰ |

---

### 5ï¸âƒ£ ç±»å‹å®‰å…¨ï¼šè¾“å…¥è¾“å‡ºæ¨¡å¼æ¨æ–­

**æºç ä½ç½®ï¼š** `libs/core/langchain_core/runnables/base.py:3694-3743`

#### è¾“å…¥æ¨¡å¼æ¨æ–­

```python
def get_input_schema(self, config: RunnableConfig | None = None) -> type[BaseModel]:
    # å¦‚æœæ‰€æœ‰åˆ†æ”¯çš„è¾“å…¥éƒ½æ˜¯å¯¹è±¡ç±»å‹
    if all(
        s.get_input_schema(config).model_json_schema().get("type") == "object"
        for s in self.steps__.values()
    ):
        # åˆå¹¶æ‰€æœ‰åˆ†æ”¯çš„è¾“å…¥å­—æ®µ
        return create_model_v2(
            self.get_name("Input"),
            field_definitions={
                k: (v.annotation, v.default)
                for step in self.steps__.values()
                for k, v in step.get_input_schema(config).model_fields.items()
                if k != "__root__"
            },
        )

    return super().get_input_schema(config)
```

**ç¤ºä¾‹ï¼š**
```python
parallel = RunnableParallel(
    add=RunnableLambda(lambda x: x["a"] + x["b"]),     # éœ€è¦ a, b
    multiply=RunnableLambda(lambda x: x["a"] * x["c"]), # éœ€è¦ a, c
)

# è‡ªåŠ¨æ¨æ–­è¾“å…¥æ¨¡å¼ï¼š{"a": int, "b": int, "c": int}
```

#### è¾“å‡ºæ¨¡å¼æ¨æ–­

```python
def get_output_schema(self, config: RunnableConfig | None = None) -> type[BaseModel]:
    # è¾“å‡ºï¼šæ¯ä¸ªåˆ†æ”¯çš„è¾“å‡ºç±»å‹
    fields = {k: (v.OutputType, ...) for k, v in self.steps__.items()}
    return create_model_v2(self.get_name("Output"), field_definitions=fields)
```

**ç¤ºä¾‹ï¼š**
```python
# è¾“å‡ºæ¨¡å¼ï¼š{"add": int, "multiply": int}
```

---

## ğŸ§© å®æˆ˜åº”ç”¨æ¨¡å¼

### æ¨¡å¼ 1ï¼šRAG æ£€ç´¢å¢å¼ºç”Ÿæˆ

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

rag_chain = (
    # å¹¶è¡Œé˜¶æ®µï¼šæ£€ç´¢ + ä¿ç•™åŸå§‹é—®é¢˜
    RunnableParallel(
        context=retriever,  # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        question=RunnablePassthrough()  # é€ä¼ åŸå§‹é—®é¢˜
    )
    # é¡ºåºé˜¶æ®µï¼šç»„åˆæˆæç¤º -> è°ƒç”¨æ¨¡å‹ -> è§£æè¾“å‡º
    | ChatPromptTemplate.from_template(
        "Context: {context}\n\nQuestion: {question}\n\nAnswer:"
    )
    | model
    | StrOutputParser()
)

result = rag_chain.invoke("What is LangChain?")
```

**æµç¨‹å›¾ï¼š**
```
è¾“å…¥: "What is LangChain?"
    â†“
RunnableParallel
    â”œâ†’ retriever       â†’ ["LangChain is...", "It helps..."]
    â””â†’ RunnablePassthrough â†’ "What is LangChain?"
    â†“
{"context": [...], "question": "..."}
    â†“
ChatPromptTemplate â†’ [HumanMessage("Context: ...\n\nQuestion: ...")]
    â†“
model â†’ AIMessage("LangChain is a framework...")
    â†“
StrOutputParser â†’ "LangChain is a framework..."
```

### æ¨¡å¼ 2ï¼šå¤šæ¨¡å‹å¯¹æ¯”

```python
comparison_chain = RunnableParallel(
    claude=claude_prompt | claude_model | parser,
    gpt=gpt_prompt | gpt_model | parser,
    llama=llama_prompt | llama_model | parser,
)

results = comparison_chain.invoke({"topic": "AI ethics"})
# {
#     "claude": "Claude's response...",
#     "gpt": "GPT's response...",
#     "llama": "Llama's response..."
# }
```

### æ¨¡å¼ 3ï¼šåµŒå¥—å¹¶è¡Œ

```python
# å¤–å±‚ï¼šä¸åŒç±»å‹çš„åˆ†æ
outer_parallel = RunnableParallel(
    sentiment=sentiment_chain,

    # å†…å±‚ï¼šå¤šä¸ªå®ä½“è¯†åˆ«æ¨¡å‹å¹¶è¡Œ
    entities=RunnableParallel(
        spacy_entities=spacy_ner,
        transformers_entities=transformers_ner,
    ),

    summary=summarization_chain,
)
```

**æ•°æ®æµï¼š**
```
Input: "Apple released new iPhone..."
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OuterParallel                  â”‚
â”‚  â”œâ”€ sentiment                   â”‚
â”‚  â”œâ”€ entities (InnerParallel)    â”‚
â”‚  â”‚   â”œâ”€ spacy_entities          â”‚
â”‚  â”‚   â””â”€ transformers_entities   â”‚
â”‚  â””â”€ summary                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
{
    "sentiment": "positive",
    "entities": {
        "spacy_entities": ["Apple", "iPhone"],
        "transformers_entities": ["Apple Inc.", "iPhone 15"]
    },
    "summary": "Apple announced..."
}
```

---

## ğŸ” è®¾è®¡å“²å­¦ä¸æƒè¡¡

### âœ… ä¼˜ç‚¹

1. **æ€§èƒ½æå‡**
   - I/O å¯†é›†å‹ä»»åŠ¡ï¼šçº¿ç¨‹æ± å¹¶è¡Œï¼ˆ`invoke()`ï¼‰
   - å¼‚æ­¥ä»»åŠ¡ï¼šåç¨‹å¹¶å‘ï¼ˆ`ainvoke()`ï¼‰

2. **ç®€æ´è¯­æ³•**
   - å­—å…¸è¯­æ³•ï¼š`{"key": runnable}` è‡ªåŠ¨åˆ›å»º `RunnableParallel`
   - æ— éœ€æ‰‹åŠ¨ç®¡ç†çº¿ç¨‹æˆ–åç¨‹

3. **ç»Ÿä¸€æ¥å£**
   - ä»ç„¶æ˜¯ `Runnable`ï¼Œæ”¯æŒ `invoke()`ã€`batch()`ã€`stream()` ç­‰

4. **å¯ç»„åˆæ€§**
   - å¯åµŒå¥—åœ¨ `RunnableSequence` ä¸­
   - å¯ä½œä¸ºå…¶ä»– `RunnableParallel` çš„åˆ†æ”¯

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **å…±äº«è¾“å…¥**
   - æ‰€æœ‰åˆ†æ”¯æ¥æ”¶**å®Œå…¨ç›¸åŒ**çš„è¾“å…¥
   - å¦‚éœ€ä¸åŒè¾“å…¥ï¼Œä½¿ç”¨ `RunnableLambda` é¢„å¤„ç†

2. **å¹¶å‘å®‰å…¨**
   - é¿å…ä¿®æ”¹å¯å˜å…±äº«çŠ¶æ€
   - æºç é€šè¿‡ `dict(self.steps__)` æµ…æ‹·è´é¿å…å¹¶å‘ä¿®æ”¹

3. **æ€§èƒ½è€ƒé‡**
   - CPU å¯†é›†å‹ä»»åŠ¡ï¼šçº¿ç¨‹æ± å— GIL é™åˆ¶ï¼Œè€ƒè™‘è¿›ç¨‹æ± 
   - åˆ†æ”¯è¿‡å¤šï¼šçº¿ç¨‹/åç¨‹å¼€é”€å¯èƒ½æŠµæ¶ˆæ”¶ç›Š

4. **é”™è¯¯å¤„ç†**
   - ä»»ä¸€åˆ†æ”¯å¤±è´¥ä¼šå¯¼è‡´æ•´ä¸ª `invoke()` å¤±è´¥
   - éœ€è¦åœ¨åˆ†æ”¯å†…éƒ¨å¤„ç†å¼‚å¸¸æˆ–ä½¿ç”¨ `with_fallbacks()`

---

## ğŸ§  çŸ¥è¯†æ£€éªŒ

### é—®é¢˜ 1ï¼šè¾“å‡ºç±»å‹
```python
parallel = RunnableParallel(
    double=RunnableLambda(lambda x: x * 2),
    square=RunnableLambda(lambda x: x ** 2),
)
result = parallel.invoke(3)
```
**`result` çš„å€¼å’Œç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ**

<details>
<summary>ç­”æ¡ˆ</summary>

**å€¼ï¼š** `{"double": 6, "square": 9}`
**ç±»å‹ï¼š** `dict[str, Any]`

**è§£é‡Šï¼š** `RunnableParallel` æ€»æ˜¯è¿”å›å­—å…¸ï¼Œé”®ä¸ºåˆ†æ”¯åï¼Œå€¼ä¸ºå„åˆ†æ”¯è¾“å‡ºã€‚
</details>

### é—®é¢˜ 2ï¼šæ‰§è¡Œé¡ºåº
```python
def log_and_return(name):
    def fn(x):
        print(f"{name}: {x}")
        time.sleep(1)
        return x * 2
    return RunnableLambda(fn)

parallel = RunnableParallel(
    a=log_and_return("A"),
    b=log_and_return("B"),
    c=log_and_return("C"),
)
parallel.invoke(5)
```
**æ‰“å°é¡ºåºæ˜¯å›ºå®šçš„å—ï¼Ÿæ€»è€—æ—¶æ˜¯å¤šå°‘ï¼Ÿ**

<details>
<summary>ç­”æ¡ˆ</summary>

**æ‰“å°é¡ºåºï¼š** ä¸å›ºå®šï¼ˆå¹¶å‘æ‰§è¡Œï¼Œé¡ºåºä¸ç¡®å®šï¼‰
**æ€»è€—æ—¶ï¼š** çº¦ 1 ç§’ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼Œè€—æ—¶ = max(å„åˆ†æ”¯è€—æ—¶)ï¼‰

**è§£é‡Šï¼š** ä¸‰ä¸ªåˆ†æ”¯åœ¨çº¿ç¨‹æ± ä¸­å¹¶å‘æ‰§è¡Œï¼Œæ‰“å°é¡ºåºå–å†³äºçº¿ç¨‹è°ƒåº¦ã€‚
</details>

### é—®é¢˜ 3ï¼šå®ç° RAG
**ä»»åŠ¡ï¼š** ä½¿ç”¨ `RunnableParallel` å®ç°ä»¥ä¸‹ RAG æµç¨‹ï¼š
1. è¾“å…¥ç”¨æˆ·é—®é¢˜
2. å¹¶è¡Œæ‰§è¡Œï¼š
   - æ£€ç´¢ top-3 æ–‡æ¡£ï¼ˆç”¨ `mock_retriever` æ¨¡æ‹Ÿï¼‰
   - ä¿ç•™åŸå§‹é—®é¢˜
3. å°†ç»“æœä¼ é€’ç»™ä¸‹æ¸¸

<details>
<summary>ç­”æ¡ˆ</summary>

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda

def mock_retriever(question):
    """æ¨¡æ‹Ÿæ£€ç´¢å™¨"""
    return ["Doc1: LangChain is...", "Doc2: LCEL means...", "Doc3: Runnable is..."]

rag_parallel = RunnableParallel(
    context=RunnableLambda(mock_retriever),
    question=RunnablePassthrough()
)

result = rag_parallel.invoke("What is LangChain?")
# {
#     "context": ["Doc1: ...", "Doc2: ...", "Doc3: ..."],
#     "question": "What is LangChain?"
# }
```

**å…³é”®ç‚¹ï¼š**
- `RunnablePassthrough()`ï¼šä¿ç•™åŸå§‹è¾“å…¥
- `RunnableLambda(mock_retriever)`ï¼šåŒ…è£…æ™®é€šå‡½æ•°ä¸º Runnable
</details>

---

## ğŸ“š ç›¸å…³é“¾æ¥

- **å‰ç½®æ¨¡å—ï¼š** [æ¨¡å— 2 - RunnableSequence](module-02-runnable-sequence-ZH.md)
- **ä¸‹ä¸€æ¨¡å—ï¼š** [æ¨¡å— 8 - RunnableLambda](module-08-runnable-lambda-ZH.md)
- **ä»£ç ç¤ºä¾‹ï¼š** [examples/03_runnable_parallel.py](examples/03_runnable_parallel.py)
- **æœ¯è¯­è¡¨ï¼š** [GLOSSARY.md](GLOSSARY.md#runnableparallel--å¹¶è¡Œå¯è¿è¡Œç»„ä»¶)

---

**å­¦ä¹ è¿›åº¦ï¼š** âœ… å·²å®Œæˆ Runnable æ ¸å¿ƒæŠ½è±¡ã€RunnableSequenceã€RunnableParallel

**ä¸‹ä¸€æ­¥ï¼š** å­¦ä¹  `RunnableLambda` - å¦‚ä½•å°†ä»»æ„ Python å‡½æ•°é›†æˆåˆ° LCEL é“¾ä¸­
