# â“ Frequently Asked Questions (FAQ) | å¸¸è§é—®é¢˜è§£ç­”

A bilingual collection of frequently asked questions about LangChain's core concepts and architecture.

å…³äº LangChain æ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„çš„å¸¸è§é—®é¢˜åŒè¯­é›†åˆã€‚

---

## ğŸ“š Table of Contents | ç›®å½•

### Runnable & LCEL
1. [What's the difference between `invoke()` and `__call__()`?](#q1-invoke-vs-call)
2. [When should I use RunnableSequence vs RunnableParallel?](#q2-sequence-vs-parallel)
3. [Can I use regular Python functions in LCEL chains?](#q3-python-functions-in-lcel)
4. [How does the `|` operator work?](#q4-pipe-operator)

### Prompts
5. [PromptTemplate vs ChatPromptTemplate - which to use?](#q5-prompt-templates)
6. [How do I handle conversation history?](#q6-conversation-history)
7. [What are partial_variables used for?](#q7-partial-variables)

### Models
8. [BaseLLM vs BaseChatModel - what's the difference?](#q8-llm-vs-chatmodel)
9. [How do I stream responses?](#q9-streaming)
10. [What is bind_tools() for?](#q10-bind-tools)

### Performance & Optimization
11. [How can I speed up my chains?](#q11-performance)
12. [When should I use batch() vs individual invoke()?](#q12-batch-vs-invoke)
13. [Does RunnableParallel really run in parallel?](#q13-parallel-execution)

### Error Handling & Debugging
14. [How do I debug a chain?](#q14-debugging)
15. [How to handle errors in chains?](#q15-error-handling)
16. [Why is my chain not working as expected?](#q16-chain-not-working)

---

## Runnable & LCEL

<a name="q1-invoke-vs-call"></a>
### 1. What's the difference between `invoke()` and `__call__()`?
### 1. `invoke()` å’Œ `__call__()` æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**English:**
- `invoke()` is the **official method** defined in the Runnable interface. It accepts a `config` parameter for callbacks, tags, and metadata.
- `__call__()` is a **convenience wrapper** that internally calls `invoke()`. It doesn't support `config`.

**Use `invoke()`** when you need to pass configuration (callbacks, tags, etc.).

**ä¸­æ–‡ï¼š**
- `invoke()` æ˜¯ Runnable æ¥å£ä¸­å®šä¹‰çš„**å®˜æ–¹æ–¹æ³•**ã€‚å®ƒæ¥å— `config` å‚æ•°ç”¨äºå›è°ƒã€æ ‡ç­¾å’Œå…ƒæ•°æ®ã€‚
- `__call__()` æ˜¯ä¸€ä¸ª**ä¾¿æ·åŒ…è£…å™¨**ï¼Œå†…éƒ¨è°ƒç”¨ `invoke()`ã€‚å®ƒä¸æ”¯æŒ `config`ã€‚

**å½“éœ€è¦ä¼ é€’é…ç½®æ—¶ä½¿ç”¨ `invoke()`**ï¼ˆå›è°ƒã€æ ‡ç­¾ç­‰ï¼‰ã€‚

**Example:**
```python
# Both work
result1 = chain.invoke(input)
result2 = chain(input)

# Only invoke() supports config
result3 = chain.invoke(input, config={"callbacks": [my_callback]})
```

**Code Reference:** `libs/core/langchain_core/runnables/base.py:1289-1292`

---

<a name="q2-sequence-vs-parallel"></a>
### 2. When should I use RunnableSequence vs RunnableParallel?
### 2. ä½•æ—¶ä½¿ç”¨ RunnableSequence è¿˜æ˜¯ RunnableParallelï¼Ÿ

**English:**
- **RunnableSequence (`|`)**: When output of one step becomes input of the next (sequential dependency)
- **RunnableParallel (`{}`)**: When multiple operations need the same input and you want them to run concurrently

**ä¸­æ–‡ï¼š**
- **RunnableSequence (`|`)**ï¼šå½“ä¸€æ­¥çš„è¾“å‡ºæˆä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥æ—¶ï¼ˆé¡ºåºä¾èµ–ï¼‰
- **RunnableParallel (`{}`)**ï¼šå½“å¤šä¸ªæ“ä½œéœ€è¦ç›¸åŒè¾“å…¥ä¸”ä½ å¸Œæœ›å®ƒä»¬å¹¶å‘è¿è¡Œæ—¶

**Example:**
```python
# Sequential: output flows through steps
chain = prompt | model | parser  # prompt output â†’ model input â†’ parser input

# Parallel: same input to all branches
parallel = {
    "summary": summarize_chain,
    "translation": translate_chain
}  # Both chains receive the same input
```

---

<a name="q3-python-functions-in-lcel"></a>
### 3. Can I use regular Python functions in LCEL chains?
### 3. å¯ä»¥åœ¨ LCEL é“¾ä¸­ä½¿ç”¨æ™®é€š Python å‡½æ•°å—ï¼Ÿ

**English:**
Yes! LangChain automatically converts functions to `RunnableLambda`:

1. **Automatic conversion** in dict syntax:
   ```python
   chain = {"result": lambda x: x * 2} | other_runnable
   ```

2. **Explicit wrapping** with `RunnableLambda`:
   ```python
   from langchain_core.runnables import RunnableLambda

   def my_function(x):
       return x.upper()

   chain = prompt | RunnableLambda(my_function) | model
   ```

**ä¸­æ–‡ï¼š**
å¯ä»¥ï¼LangChain ä¼šè‡ªåŠ¨å°†å‡½æ•°è½¬æ¢ä¸º `RunnableLambda`ï¼š

1. **è‡ªåŠ¨è½¬æ¢**ï¼ˆå­—å…¸è¯­æ³•ï¼‰ï¼š
   ```python
   chain = {"result": lambda x: x * 2} | other_runnable
   ```

2. **æ˜¾å¼åŒ…è£…**ï¼ˆä½¿ç”¨ `RunnableLambda`ï¼‰ï¼š
   ```python
   from langchain_core.runnables import RunnableLambda

   def my_function(x):
       return x.upper()

   chain = prompt | RunnableLambda(my_function) | model
   ```

---

<a name="q4-pipe-operator"></a>
### 4. How does the `|` operator work?
### 4. `|` æ“ä½œç¬¦æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

**English:**
The `|` operator is overloaded via the `__or__()` method in the Runnable class. It creates a `RunnableSequence`:

```python
a | b  â†’  RunnableSequence(first=a, last=b)
a | b | c  â†’  RunnableSequence(first=a, middle=[b], last=c)
```

**Flattening optimization:** If you pipe two sequences, LangChain flattens them into one instead of nesting.

**ä¸­æ–‡ï¼š**
`|` æ“ä½œç¬¦é€šè¿‡ Runnable ç±»ä¸­çš„ `__or__()` æ–¹æ³•é‡è½½ã€‚å®ƒåˆ›å»ºä¸€ä¸ª `RunnableSequence`ï¼š

```python
a | b  â†’  RunnableSequence(first=a, last=b)
a | b | c  â†’  RunnableSequence(first=a, middle=[b], last=c)
```

**æ‰å¹³åŒ–ä¼˜åŒ–ï¼š** å¦‚æœä½ ç®¡é“ä¸¤ä¸ªåºåˆ—ï¼ŒLangChain ä¼šå°†å®ƒä»¬æ‰å¹³åŒ–ä¸ºä¸€ä¸ªè€Œä¸æ˜¯åµŒå¥—ã€‚

**Code Reference:** `libs/core/langchain_core/runnables/base.py:1165-1200`

---

## Prompts

<a name="q5-prompt-templates"></a>
### 5. PromptTemplate vs ChatPromptTemplate - which to use?
### 5. PromptTemplate è¿˜æ˜¯ ChatPromptTemplate - è¯¥ç”¨å“ªä¸ªï¼Ÿ

**English:**
| Template | Output | Use When |
|----------|--------|----------|
| **PromptTemplate** | String (`StringPromptValue`) | Legacy LLMs (text in â†’ text out) |
| **ChatPromptTemplate** | Message list (`ChatPromptValue`) | Modern chat models (messages in â†’ message out) |

**Recommendation:** Use `ChatPromptTemplate` for all new projects with chat models (ChatOpenAI, ChatAnthropic, etc.).

**ä¸­æ–‡ï¼š**
| æ¨¡æ¿ | è¾“å‡º | ä½¿ç”¨åœºæ™¯ |
|------|------|----------|
| **PromptTemplate** | å­—ç¬¦ä¸² (`StringPromptValue`) | ä¼ ç»Ÿ LLMï¼ˆæ–‡æœ¬è¾“å…¥ â†’ æ–‡æœ¬è¾“å‡ºï¼‰ |
| **ChatPromptTemplate** | æ¶ˆæ¯åˆ—è¡¨ (`ChatPromptValue`) | ç°ä»£èŠå¤©æ¨¡å‹ï¼ˆæ¶ˆæ¯è¾“å…¥ â†’ æ¶ˆæ¯è¾“å‡ºï¼‰ |

**å»ºè®®ï¼š** å¯¹äºæ‰€æœ‰ä½¿ç”¨èŠå¤©æ¨¡å‹çš„æ–°é¡¹ç›®ä½¿ç”¨ `ChatPromptTemplate`ï¼ˆChatOpenAIã€ChatAnthropic ç­‰ï¼‰ã€‚

**Example:**
```python
# PromptTemplate (old-school)
prompt = PromptTemplate.from_template("Tell me about {topic}")
prompt.invoke({"topic": "AI"})  # â†’ StringPromptValue

# ChatPromptTemplate (modern)
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "Tell me about {topic}")
])
prompt.invoke({"topic": "AI"})  # â†’ ChatPromptValue
```

---

<a name="q6-conversation-history"></a>
### 6. How do I handle conversation history?
### 6. å¦‚ä½•å¤„ç†å¯¹è¯å†å²ï¼Ÿ

**English:**
Use **MessagesPlaceholder** to dynamically inject conversation history:

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder("history"),  # Dynamic history insertion
    ("human", "{question}"),
])

prompt.invoke({
    "history": [
        ("human", "What's 2+2?"),
        ("ai", "2+2 equals 4."),
    ],
    "question": "What about 2+3?"
})
```

**ä¸­æ–‡ï¼š**
ä½¿ç”¨ **MessagesPlaceholder** åŠ¨æ€æ³¨å…¥å¯¹è¯å†å²ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder("history"),  # åŠ¨æ€å†å²æ’å…¥
    ("human", "{question}"),
])

prompt.invoke({
    "history": [
        ("human", "What's 2+2?"),
        ("ai", "2+2 equals 4."),
    ],
    "question": "What about 2+3?"
})
```

**See Also:** [Module 3 - Prompts Implementation](module-03-prompts-implementation-EN.md)

---

<a name="q7-partial-variables"></a>
### 7. What are partial_variables used for?
### 7. partial_variables ç”¨äºä»€ä¹ˆï¼Ÿ

**English:**
**partial_variables** allow you to pre-fill template variables with:
1. **Static values** (constants)
2. **Functions** (evaluated lazily on each invoke)

**Common use cases:**
- Current timestamp
- User ID from context
- System version info

**ä¸­æ–‡ï¼š**
**partial_variables** å…è®¸ä½ é¢„å¡«å……æ¨¡æ¿å˜é‡ï¼Œä½¿ç”¨ï¼š
1. **é™æ€å€¼**ï¼ˆå¸¸é‡ï¼‰
2. **å‡½æ•°**ï¼ˆæ¯æ¬¡è°ƒç”¨æ—¶æƒ°æ€§æ±‚å€¼ï¼‰

**å¸¸è§ç”¨ä¾‹ï¼š**
- å½“å‰æ—¶é—´æˆ³
- ä¸Šä¸‹æ–‡ä¸­çš„ç”¨æˆ· ID
- ç³»ç»Ÿç‰ˆæœ¬ä¿¡æ¯

**Example:**
```python
from datetime import datetime

prompt = PromptTemplate(
    template="[{time}] User question: {question}",
    input_variables=["question"],
    partial_variables={
        "time": lambda: datetime.now().strftime("%H:%M:%S")  # Called on each invoke
    }
)

prompt.format(question="What is AI?")
# [14:23:45] User question: What is AI?
```

**Code Reference:** `libs/core/langchain_core/prompts/base.py`

---

## Models

<a name="q8-llm-vs-chatmodel"></a>
### 8. BaseLLM vs BaseChatModel - what's the difference?
### 8. BaseLLM å’Œ BaseChatModel æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**English:**
| Feature | BaseLLM | BaseChatModel |
|---------|---------|---------------|
| **Input** | String | List of messages |
| **Output** | String | AIMessage |
| **Use Case** | Legacy completion models | Modern chat models (GPT-4, Claude, etc.) |
| **Example** | Text completion | Conversational AI |

**Recommendation:** Use `BaseChatModel` for new projects.

**ä¸­æ–‡ï¼š**
| ç‰¹æ€§ | BaseLLM | BaseChatModel |
|------|---------|---------------|
| **è¾“å…¥** | å­—ç¬¦ä¸² | æ¶ˆæ¯åˆ—è¡¨ |
| **è¾“å‡º** | å­—ç¬¦ä¸² | AIMessage |
| **ç”¨ä¾‹** | ä¼ ç»Ÿè¡¥å…¨æ¨¡å‹ | ç°ä»£èŠå¤©æ¨¡å‹ï¼ˆGPT-4ã€Claude ç­‰ï¼‰ |
| **ç¤ºä¾‹** | æ–‡æœ¬è¡¥å…¨ | å¯¹è¯å¼ AI |

**å»ºè®®ï¼š** æ–°é¡¹ç›®ä½¿ç”¨ `BaseChatModel`ã€‚

**See Also:** [Module 4-6 Summary](module-04-05-06-summary-EN.md)

---

<a name="q9-streaming"></a>
### 9. How do I stream responses?
### 9. å¦‚ä½•æµå¼ä¼ è¾“å“åº”ï¼Ÿ

**English:**
Use the `stream()` method to get incremental output:

```python
for chunk in model.stream("Tell me a long story"):
    print(chunk.content, end="", flush=True)
```

**For chains:**
```python
chain = prompt | model | StrOutputParser()

for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)
```

**ä¸­æ–‡ï¼š**
ä½¿ç”¨ `stream()` æ–¹æ³•è·å–å¢é‡è¾“å‡ºï¼š

```python
for chunk in model.stream("Tell me a long story"):
    print(chunk.content, end="", flush=True)
```

**å¯¹äºé“¾ï¼š**
```python
chain = prompt | model | StrOutputParser()

for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)
```

---

<a name="q10-bind-tools"></a>
### 10. What is bind_tools() for?
### 10. bind_tools() ç”¨äºä»€ä¹ˆï¼Ÿ

**English:**
`bind_tools()` enables **function calling** - allowing the LLM to generate structured calls to external tools/functions.

**ä¸­æ–‡ï¼š**
`bind_tools()` å¯ç”¨**å‡½æ•°è°ƒç”¨** - å…è®¸ LLM ç”Ÿæˆå¯¹å¤–éƒ¨å·¥å…·/å‡½æ•°çš„ç»“æ„åŒ–è°ƒç”¨ã€‚

**Example:**
```python
from langchain_core.tools import tool

@tool
def get_weather(location: str) -> str:
    """Get weather for a location."""
    return f"Weather in {location}: Sunny"

model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke("What's the weather in Paris?")
print(response.tool_calls)
# [{"name": "get_weather", "args": {"location": "Paris"}, "id": "..."}]
```

**See Also:** [GLOSSARY.md - Function Calling](GLOSSARY.md#function-calling--å‡½æ•°è°ƒç”¨)

---

## Performance & Optimization

<a name="q11-performance"></a>
### 11. How can I speed up my chains?
### 11. å¦‚ä½•åŠ é€Ÿæˆ‘çš„é“¾ï¼Ÿ

**English:**
1. **Use RunnableParallel** for independent operations
2. **Enable caching** with `set_llm_cache()`
3. **Batch processing** with `batch()` for multiple inputs
4. **Async execution** with `ainvoke()` / `abatch()`
5. **Optimize prompts** to reduce token usage

**ä¸­æ–‡ï¼š**
1. **å¯¹ç‹¬ç«‹æ“ä½œä½¿ç”¨ RunnableParallel**
2. **ä½¿ç”¨ `set_llm_cache()` å¯ç”¨ç¼“å­˜**
3. **ä½¿ç”¨ `batch()` æ‰¹å¤„ç†**å¤šä¸ªè¾“å…¥
4. **ä½¿ç”¨ `ainvoke()` / `abatch()` å¼‚æ­¥æ‰§è¡Œ**
5. **ä¼˜åŒ–æç¤º**ä»¥å‡å°‘ä»¤ç‰Œä½¿ç”¨

**Example:**
```python
# Slow: Sequential execution
summary = summarize.invoke(text)
translation = translate.invoke(text)

# Fast: Parallel execution
parallel = RunnableParallel(
    summary=summarize,
    translation=translate
)
result = parallel.invoke(text)  # Both run concurrently
```

---

<a name="q12-batch-vs-invoke"></a>
### 12. When should I use batch() vs individual invoke()?
### 12. ä½•æ—¶ä½¿ç”¨ batch() è€Œéå•ç‹¬çš„ invoke()ï¼Ÿ

**English:**
Use `batch()` when:
- âœ… You have multiple independent inputs to process
- âœ… API supports batch requests (efficiency gains)
- âœ… You want automatic parallelization

Use individual `invoke()` when:
- âœ… Inputs depend on previous results
- âœ… You need fine-grained control over execution

**ä¸­æ–‡ï¼š**
ä½¿ç”¨ `batch()` å½“ï¼š
- âœ… æœ‰å¤šä¸ªç‹¬ç«‹è¾“å…¥è¦å¤„ç†
- âœ… API æ”¯æŒæ‰¹å¤„ç†è¯·æ±‚ï¼ˆæ•ˆç‡æå‡ï¼‰
- âœ… å¸Œæœ›è‡ªåŠ¨å¹¶è¡ŒåŒ–

ä½¿ç”¨å•ç‹¬çš„ `invoke()` å½“ï¼š
- âœ… è¾“å…¥ä¾èµ–äºä¹‹å‰çš„ç»“æœ
- âœ… éœ€è¦å¯¹æ‰§è¡Œè¿›è¡Œç»†ç²’åº¦æ§åˆ¶

**Example:**
```python
# Batch processing (efficient)
inputs = ["doc1", "doc2", "doc3"]
results = chain.batch(inputs)  # Processes all in parallel/batch

# Individual processing (when needed)
results = []
for inp in inputs:
    result = chain.invoke(inp)  # Sequential, controlled
    if result.score > 0.8:  # Can make decisions between invocations
        results.append(result)
```

---

<a name="q13-parallel-execution"></a>
### 13. Does RunnableParallel really run in parallel?
### 13. RunnableParallel çœŸçš„å¹¶è¡Œè¿è¡Œå—ï¼Ÿ

**English:**
**It depends:**

| Method | Parallelism Type | True Parallel? |
|--------|------------------|----------------|
| `invoke()` | Thread pool (`ThreadPoolExecutor`) | âœ… Yes (for I/O-bound tasks) |
| `ainvoke()` | Async coroutines (`asyncio.gather`) | âœ… Yes (for async I/O) |
| Python GIL | Limitation | âš ï¸ Not for CPU-bound tasks |

**CPU-bound tasks:** Use `ProcessPoolExecutor` instead.

**ä¸­æ–‡ï¼š**
**å–å†³äºæƒ…å†µï¼š**

| æ–¹æ³• | å¹¶è¡Œç±»å‹ | çœŸæ­£å¹¶è¡Œï¼Ÿ |
|------|----------|------------|
| `invoke()` | çº¿ç¨‹æ±  (`ThreadPoolExecutor`) | âœ… æ˜¯ï¼ˆå¯¹äº I/O å¯†é›†å‹ä»»åŠ¡ï¼‰ |
| `ainvoke()` | å¼‚æ­¥åç¨‹ (`asyncio.gather`) | âœ… æ˜¯ï¼ˆå¯¹äºå¼‚æ­¥ I/Oï¼‰ |
| Python GIL | é™åˆ¶ | âš ï¸ å¯¹äº CPU å¯†é›†å‹ä»»åŠ¡ä¸é€‚ç”¨ |

**CPU å¯†é›†å‹ä»»åŠ¡ï¼š** ä½¿ç”¨ `ProcessPoolExecutor`ã€‚

**See Also:** [Module 7 - RunnableParallel](module-07-runnable-parallel-EN.md)

---

## Error Handling & Debugging

<a name="q14-debugging"></a>
### 14. How do I debug a chain?
### 14. å¦‚ä½•è°ƒè¯•é“¾ï¼Ÿ

**English:**
**Method 1: Enable verbose output**
```python
chain = prompt | model
result = chain.invoke(input, config={"verbose": True})
```

**Method 2: Use callbacks**
```python
from langchain.callbacks import StdOutCallbackHandler

chain.invoke(input, config={"callbacks": [StdOutCallbackHandler()]})
```

**Method 3: LangSmith (production)**
- Sign up at https://www.langchain.com/langsmith
- Enable tracing with environment variables

**ä¸­æ–‡ï¼š**
**æ–¹æ³• 1ï¼šå¯ç”¨è¯¦ç»†è¾“å‡º**
```python
chain = prompt | model
result = chain.invoke(input, config={"verbose": True})
```

**æ–¹æ³• 2ï¼šä½¿ç”¨å›è°ƒ**
```python
from langchain.callbacks import StdOutCallbackHandler

chain.invoke(input, config={"callbacks": [StdOutCallbackHandler()]})
```

**æ–¹æ³• 3ï¼šLangSmithï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰**
- åœ¨ https://www.langchain.com/langsmith æ³¨å†Œ
- ä½¿ç”¨ç¯å¢ƒå˜é‡å¯ç”¨è¿½è¸ª

---

<a name="q15-error-handling"></a>
### 15. How to handle errors in chains?
### 15. å¦‚ä½•åœ¨é“¾ä¸­å¤„ç†é”™è¯¯ï¼Ÿ

**English:**
**Option 1: try-except**
```python
try:
    result = chain.invoke(input)
except Exception as e:
    print(f"Chain failed: {e}")
    result = fallback_chain.invoke(input)
```

**Option 2: with_fallbacks()**
```python
chain_with_fallback = primary_chain.with_fallbacks([backup_chain])
result = chain_with_fallback.invoke(input)  # Automatically uses backup if primary fails
```

**Option 3: with_retry()**
```python
chain = (prompt | model).with_retry(stop_after_attempt=3)
```

**ä¸­æ–‡ï¼š**
**é€‰é¡¹ 1ï¼štry-except**
```python
try:
    result = chain.invoke(input)
except Exception as e:
    print(f"Chain failed: {e}")
    result = fallback_chain.invoke(input)
```

**é€‰é¡¹ 2ï¼šwith_fallbacks()**
```python
chain_with_fallback = primary_chain.with_fallbacks([backup_chain])
result = chain_with_fallback.invoke(input)  # ä¸»é“¾å¤±è´¥æ—¶è‡ªåŠ¨ä½¿ç”¨å¤‡ç”¨é“¾
```

**é€‰é¡¹ 3ï¼šwith_retry()**
```python
chain = (prompt | model).with_retry(stop_after_attempt=3)
```

---

<a name="q16-chain-not-working"></a>
### 16. Why is my chain not working as expected?
### 16. ä¸ºä»€ä¹ˆæˆ‘çš„é“¾æ²¡æœ‰æŒ‰é¢„æœŸå·¥ä½œï¼Ÿ

**English:**
**Common issues:**

1. **Type mismatch**
   - Check if output type of step N matches input type of step N+1
   - Use `chain.get_input_schema()` and `chain.get_output_schema()`

2. **Wrong operator**
   - `|` for sequential (output â†’ input)
   - `{}` for parallel (same input to all)

3. **Missing input variables**
   - Ensure all template variables are provided in `invoke()`

4. **Config not propagating**
   - Use `invoke(input, config={...})`, not `invoke(input, {...})`

**Debug checklist:**
```python
# 1. Check schemas
print(chain.get_input_schema())
print(chain.get_output_schema())

# 2. Test each step individually
step1_output = step1.invoke(input)
step2_output = step2.invoke(step1_output)  # Does this work?

# 3. Enable verbose logging
chain.invoke(input, config={"verbose": True})
```

**ä¸­æ–‡ï¼š**
**å¸¸è§é—®é¢˜ï¼š**

1. **ç±»å‹ä¸åŒ¹é…**
   - æ£€æŸ¥æ­¥éª¤ N çš„è¾“å‡ºç±»å‹æ˜¯å¦ä¸æ­¥éª¤ N+1 çš„è¾“å…¥ç±»å‹åŒ¹é…
   - ä½¿ç”¨ `chain.get_input_schema()` å’Œ `chain.get_output_schema()`

2. **é”™è¯¯çš„æ“ä½œç¬¦**
   - `|` ç”¨äºé¡ºåºï¼ˆè¾“å‡º â†’ è¾“å…¥ï¼‰
   - `{}` ç”¨äºå¹¶è¡Œï¼ˆç›¸åŒè¾“å…¥åˆ°æ‰€æœ‰åˆ†æ”¯ï¼‰

3. **ç¼ºå°‘è¾“å…¥å˜é‡**
   - ç¡®ä¿åœ¨ `invoke()` ä¸­æä¾›æ‰€æœ‰æ¨¡æ¿å˜é‡

4. **é…ç½®æœªä¼ æ’­**
   - ä½¿ç”¨ `invoke(input, config={...})`ï¼Œè€Œé `invoke(input, {...})`

**è°ƒè¯•æ¸…å•ï¼š**
```python
# 1. æ£€æŸ¥æ¨¡å¼
print(chain.get_input_schema())
print(chain.get_output_schema())

# 2. å•ç‹¬æµ‹è¯•æ¯ä¸ªæ­¥éª¤
step1_output = step1.invoke(input)
step2_output = step2.invoke(step1_output)  # è¿™èƒ½å·¥ä½œå—ï¼Ÿ

# 3. å¯ç”¨è¯¦ç»†æ—¥å¿—
chain.invoke(input, config={"verbose": True})
```

---

## ğŸ“š Additional Resources | å…¶ä»–èµ„æº

- **[Official Documentation](https://docs.langchain.com/)** - Complete guides and API reference
- **[Learning Series](README.md)** - Deep-dive modules on core concepts
- **[Examples Directory](examples/)** - Runnable code examples
- **[Glossary](GLOSSARY.md)** - Comprehensive terminology reference

---

## ğŸ¤ Contributing | è´¡çŒ®

Have a question not covered here? Please contribute!

è¿™é‡Œæ²¡æœ‰æ¶µç›–çš„é—®é¢˜ï¼Ÿè¯·è´¡çŒ®ï¼

1. Add your question in **both English and Chinese**
2. Provide a clear, concise answer with code examples
3. Link to relevant modules or documentation
4. Keep answers beginner-friendly

---

**Last Updated:** 2025-11-17
