# ğŸ“š æ¨¡å— 4-6ï¼šLangChain å®Œæ•´æ‰§è¡Œæµç¨‹æ€»ç»“

## æ¨¡å— 4ï¼šLLM å®ç°

### æ ¸å¿ƒè¦ç‚¹
`BaseLLM` å®ç° `Runnable[LanguageModelInput, str]` æ¥å£ï¼š

```python
class BaseLLM(RunnableSerializable[LanguageModelInput, str]):
    @abstractmethod
    def _generate(self, prompts: List[str], stop: List[str] | None = None) -> LLMResult:
        """æ ¸å¿ƒç”Ÿæˆæ–¹æ³•"""
    
    def invoke(self, input: LanguageModelInput, config: RunnableConfig | None = None) -> str:
        """è°ƒç”¨é“¾ï¼šinvoke â†’ _generate_helper â†’ _generate"""
```

**å…³é”®è®¾è®¡ï¼š**
- **è¾“å…¥ç±»å‹**ï¼š`LanguageModelInput = str | List[BaseMessage] | PromptValue`
- **è¾“å‡ºç±»å‹**ï¼š`str`ï¼ˆå•ä¸ªç”Ÿæˆçš„æ–‡æœ¬ï¼‰
- **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼š`_generate` æ¥å— `List[str]` å®ç°æ‰¹å¤„ç†
- **æµå¼æ”¯æŒ**ï¼š`_stream` æ–¹æ³•é€tokenè¿”å›

---

## æ¨¡å— 5ï¼šChatModel å®ç°

### æ ¸å¿ƒè¦ç‚¹
`BaseChatModel` å®ç° `Runnable[LanguageModelInput, BaseMessage]` æ¥å£ï¼š

```python
class BaseChatModel(RunnableSerializable[LanguageModelInput, BaseMessage]):
    @abstractmethod
    def _generate(self, messages: List[BaseMessage], stop: List[str] | None = None) -> ChatResult:
        """æ ¸å¿ƒç”Ÿæˆæ–¹æ³•"""
    
    def invoke(self, input: LanguageModelInput, config: RunnableConfig | None = None) -> BaseMessage:
        """è°ƒç”¨é“¾ï¼šinvoke â†’ _generate_with_cache â†’ _generate"""
```

**å…³é”®è®¾è®¡ï¼š**
- **è¾“å…¥ç±»å‹**ï¼šåŒ LLMï¼Œä½†ä¼˜å…ˆå¤„ç†æ¶ˆæ¯åˆ—è¡¨
- **è¾“å‡ºç±»å‹**ï¼š`BaseMessage`ï¼ˆé€šå¸¸æ˜¯ `AIMessage`ï¼‰
- **å·¥å…·è°ƒç”¨**ï¼š`bind_tools()` æ–¹æ³•ç»‘å®šå·¥å…·å®šä¹‰
- **ç»“æ„åŒ–è¾“å‡º**ï¼š`with_structured_output()` å¼ºåˆ¶ç‰¹å®šæ ¼å¼

**ChatModel vs LLM å¯¹æ¯”ï¼š**

| ç‰¹æ€§ | BaseLLM | BaseChatModel |
|------|---------|---------------|
| è¾“å…¥ | å­—ç¬¦ä¸²æç¤º | æ¶ˆæ¯åˆ—è¡¨ |
| è¾“å‡º | å­—ç¬¦ä¸² | BaseMessage |
| å·¥å…·è°ƒç”¨ | âŒ | âœ… |
| æµå¼è¾“å‡º | Token-by-token | Message chunks |

---

## æ¨¡å— 6ï¼šLCEL å®Œæ•´æ‰§è¡Œæµç¨‹

### å®Œæ•´é“¾ç¤ºä¾‹

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# å®šä¹‰é“¾
chain = (
    ChatPromptTemplate.from_template("Translate {text} to {language}")
    | ChatOpenAI(model="gpt-4")
    | StrOutputParser()
)

# æ‰§è¡Œ
result = chain.invoke({"text": "Hello", "language": "French"})
```

### æ‰§è¡Œæµç¨‹å¯è§†åŒ–

```mermaid
graph LR
    Input[ç”¨æˆ·è¾“å…¥: dict] --> Prompt[ChatPromptTemplate]
    Prompt -->|PromptValue| Model[ChatOpenAI]
    Model -->|AIMessage| Parser[StrOutputParser]
    Parser -->|str| Output[æœ€ç»ˆè¾“å‡º]
    
    style Input fill:#f9f
    style Output fill:#9f9
```

### å…³é”®æœºåˆ¶

**1. ç±»å‹è‡ªåŠ¨è½¬æ¢**
```python
# æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºç±»å‹åŒ¹é…ä¸‹ä¸€æ­¥çš„è¾“å…¥ç±»å‹ï¼š
# dict â†’ PromptValue â†’ AIMessage â†’ str
```

**2. Config ä¼ é€’**
```python
chain.invoke(
    {"text": "Hello", "language": "French"},
    config={
        "tags": ["translation"],
        "metadata": {"user_id": "123"},
        "callbacks": [ConsoleCallbackHandler()],
    }
)
# config ä¼šä¼ é€’ç»™æ¯ä¸ª Runnable
```

**3. æµå¼æ‰§è¡Œ**
```python
for chunk in chain.stream({"text": "Hello", "language": "French"}):
    print(chunk, end="", flush=True)
# è¾“å‡º: B o n j o u r
```

**4. æ‰¹å¤„ç†**
```python
results = chain.batch([
    {"text": "Hello", "language": "French"},
    {"text": "Goodbye", "language": "Spanish"},
])
# results = ["Bonjour", "AdiÃ³s"]
```

### RunnableParallelï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰

```python
from langchain_core.runnables import RunnableParallel

# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªé“¾
parallel_chain = RunnableParallel(
    french=ChatPromptTemplate.from_template("Translate {text} to French") | model,
    spanish=ChatPromptTemplate.from_template("Translate {text} to Spanish") | model,
)

result = parallel_chain.invoke({"text": "Hello"})
# result = {"french": AIMessage(...), "spanish": AIMessage(...)}
```

---

## ğŸ¯ å®Œæ•´æ¶æ„æ€»ç»“

### Runnable ç”Ÿæ€ç³»ç»Ÿ

```mermaid
graph TD
    Runnable[Runnable&lt;Input, Output&gt;] --> BasePrompt[BasePromptTemplate&lt;dict, PromptValue&gt;]
    Runnable --> BaseLLM[BaseLLM&lt;LanguageModelInput, str&gt;]
    Runnable --> BaseChatModel[BaseChatModel&lt;LanguageModelInput, BaseMessage&gt;]
    Runnable --> Sequence[RunnableSequence]
    Runnable --> Parallel[RunnableParallel]
    
    BasePrompt --> PromptTemplate[PromptTemplate]
    BasePrompt --> ChatPromptTemplate[ChatPromptTemplate]
    
    BaseLLM --> OpenAI[OpenAI]
    BaseChatModel --> ChatOpenAI[ChatOpenAI]
    
    style Runnable fill:#f9f,stroke:#333,stroke-width:4px
    style Sequence fill:#9f9,stroke:#333,stroke-width:2px
    style Parallel fill:#9f9,stroke:#333,stroke-width:2px
```

### æ ¸å¿ƒåŸåˆ™å›é¡¾

1. **ç»Ÿä¸€æ¥å£**ï¼šæ‰€æœ‰ç»„ä»¶éƒ½å®ç° `Runnable[Input, Output]`
2. **ç±»å‹å®‰å…¨**ï¼šæ³›å‹ç³»ç»Ÿç¡®ä¿ç»„åˆçš„ç±»å‹åŒ¹é…
3. **è‡ªåŠ¨èƒ½åŠ›**ï¼šå®ç° `invoke` è‡ªåŠ¨è·å¾— `batch`/`stream`/`ainvoke`
4. **ç»„åˆä¼˜å…ˆ**ï¼š`|` æ“ä½œç¬¦æ— ç¼ç»„åˆä»»æ„ Runnable
5. **å¯è§‚æµ‹æ€§**ï¼šConfig å’Œ Callback ç³»ç»Ÿç»Ÿä¸€è¿½è¸ª

---

## ğŸ§  ç»¼åˆæŒ‘æˆ˜

### æŒ‘æˆ˜ 1ï¼šè®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ RAG é“¾

**éœ€æ±‚**ï¼šè®¾è®¡ä¸€ä¸ªæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼ŒåŒ…å«ï¼š
1. æ–‡æ¡£æ£€ç´¢
2. ä¸Šä¸‹æ–‡æ³¨å…¥
3. LLM ç”Ÿæˆ
4. è¾“å‡ºè§£æ

**æç¤º**ï¼šä½¿ç”¨ `RunnableParallel` å’Œ `RunnablePassthrough`

### æŒ‘æˆ˜ 2ï¼šå®ç°è‡ªå®šä¹‰ Runnable

**ä»»åŠ¡**ï¼šåˆ›å»ºä¸€ä¸ª `TranslationCache` Runnableï¼š
- è¾“å…¥ï¼š`{"text": str, "language": str}`
- è¾“å‡ºï¼š`str`ï¼ˆç¿»è¯‘ç»“æœï¼‰
- é€»è¾‘ï¼šå¦‚æœç¼“å­˜å‘½ä¸­è¿”å›ç¼“å­˜ï¼Œå¦åˆ™è°ƒç”¨ LLM

**è¦æ±‚**ï¼š
- ç»§æ‰¿ `Runnable[dict, str]`
- å®ç° `invoke` æ–¹æ³•
- æ”¯æŒ `batch`ï¼ˆæç¤ºï¼šåˆ©ç”¨é»˜è®¤å®ç°æˆ–è¦†ç›–ä¼˜åŒ–ï¼‰

### æŒ‘æˆ˜ 3ï¼šç±»å‹æ¨å¯¼

ç»™å®šä»¥ä¸‹ Runnableï¼š
```python
A: Runnable[str, int]
B: Runnable[int, List[str]]
C: Runnable[List[str], dict]
D: Runnable[dict, str]

# é—®é¢˜ï¼š
# 1. (A | B) çš„ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ
# 2. (A | B | C) çš„ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ
# 3. (A | B | C | D) çš„ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ
# 4. å¦‚æœå°è¯• (A | C)ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
```

---

## âœ… å­¦ä¹ å®Œæˆæ£€æŸ¥è¡¨

- [ ] ç†è§£ `Runnable` çš„æ ¸å¿ƒå¥‘çº¦ï¼ˆinvoke/batch/streamï¼‰
- [ ] ç†è§£ `RunnableSequence` çš„æ‰å¹³åŒ–æœºåˆ¶
- [ ] ç†è§£ `BasePromptTemplate` çš„è¾“å…¥éªŒè¯
- [ ] ç†è§£ `BaseLLM` vs `BaseChatModel` çš„åŒºåˆ«
- [ ] èƒ½å¤Ÿä½¿ç”¨ `|` æ“ä½œç¬¦ç»„åˆ Runnable
- [ ] èƒ½å¤Ÿä½¿ç”¨ `RunnableParallel` å¹¶è¡Œæ‰§è¡Œ
- [ ] ç†è§£ Config åœ¨é“¾ä¸­çš„ä¼ é€’æœºåˆ¶
- [ ] èƒ½å¤Ÿå®ç°è‡ªå®šä¹‰ Runnable

**æ­å–œï¼æ‚¨å·²ç»å®Œæˆ LangChain æ ¸å¿ƒæºç çš„æ·±åº¦åˆ†æï¼** ğŸ‰
