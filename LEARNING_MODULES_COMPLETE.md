# ğŸ§  LangChain è®¤çŸ¥å­¦ä¹ å¼•æ“ - å®Œæ•´æ•™å­¦æ¨¡å—

> [!NOTE]
> **ğŸ“š æ–°çš„æ·±åº¦å­¦ä¹ ç³»åˆ—ç°å·²æ¨å‡ºï¼**
>
> æœ¬æ–‡æ¡£æä¾› LangChain çš„**å¹¿åº¦ä¼˜å…ˆ**å­¦ä¹ è·¯å¾„ï¼Œæ¶µç›– 8 ä¸ªä¸»é¢˜çš„å¿«é€Ÿæ¦‚è§ˆã€‚
>
> å¦‚æœä½ æƒ³è¿›è¡Œ**æ·±åº¦ä¼˜å…ˆ**çš„æºç çº§å­¦ä¹ ï¼Œè¯·è®¿é—®æ–°çš„å­¦ä¹ ç³»åˆ—ï¼š
> - **[æ·±åº¦å­¦ä¹ ç³»åˆ—ï¼ˆä¸­è‹±åŒè¯­ï¼‰](docs/README.md)**
> - ç‰¹ç‚¹ï¼šé€æ–‡ä»¶æ·±å…¥åˆ†æã€è®¾è®¡å“²å­¦è§£é‡Šã€æºç å¼•ç”¨ã€Mermaid å›¾è¡¨ã€çŸ¥è¯†æŒ‘æˆ˜
> - æ¶µç›–ï¼š`Runnable` æ ¸å¿ƒæŠ½è±¡ã€`RunnableSequence` ç»„åˆæœºåˆ¶ã€Prompts å®ç°ã€LLM/ChatModelã€LCEL æ‰§è¡Œæµç¨‹
>
> **ä¸¤ç§å­¦ä¹ è·¯å¾„å¯¹æ¯”ï¼š**
> - **æœ¬æ–‡æ¡£ï¼ˆå¹¿åº¦ä¼˜å…ˆï¼‰**ï¼šå¿«é€Ÿäº†è§£ LangChain å…¨è²Œï¼Œé€‚åˆåˆå­¦è€…å…¥é—¨
> - **æ–°ç³»åˆ—ï¼ˆæ·±åº¦ä¼˜å…ˆï¼‰**ï¼šæ·±å…¥ç†è§£æ¶æ„è®¾è®¡ï¼Œé€‚åˆè¿›é˜¶å­¦ä¹ å’Œæºç è´¡çŒ®

---

æœ¬æ–‡æ¡£åŒ…å«æ‰€æœ‰ 8 ä¸ªå­¦ä¹ æ¨¡å—çš„å®Œæ•´å†…å®¹ã€‚æ¯ä¸ªæ¨¡å—éƒ½åŒ…å«ä¸Šä¸€æ¨¡å—çš„ç­”æ¡ˆï¼Œæ–¹ä¾¿ä½ å…ˆæ€è€ƒå†éªŒè¯ã€‚

---

## ğŸ“š ç›®å½•

1. [æ¨¡å— 1ï¼šé¡¹ç›®æ¶æ„ä¸æ ¸å¿ƒç†å¿µ](#æ¨¡å—-1é¡¹ç›®æ¶æ„ä¸æ ¸å¿ƒç†å¿µ)
2. [æ¨¡å— 2ï¼šæ¶ˆæ¯ç³»ç»Ÿ (Messages)](#æ¨¡å—-2æ¶ˆæ¯ç³»ç»Ÿ-messages)
3. [æ¨¡å— 3ï¼šæç¤ºå·¥ç¨‹ (Prompts)](#æ¨¡å—-3æç¤ºå·¥ç¨‹-prompts)
4. [æ¨¡å— 4ï¼šæ ¸å¿ƒæŠ½è±¡ Runnable](#æ¨¡å—-4æ ¸å¿ƒæŠ½è±¡-runnable)
5. [æ¨¡å— 5ï¼šèŠå¤©æ¨¡å‹ (Chat Models)](#æ¨¡å—-5èŠå¤©æ¨¡å‹-chat-models)
6. [æ¨¡å— 6ï¼šé“¾å¼ç»„åˆ (Chains & LCEL)](#æ¨¡å—-6é“¾å¼ç»„åˆ-chains--lcel)
7. [æ¨¡å— 7ï¼šå·¥å…·ä¸ä»£ç† (Tools & Agents)](#æ¨¡å—-7å·¥å…·ä¸ä»£ç†-tools--agents)
8. [æ¨¡å— 8ï¼šé«˜çº§ç‰¹æ€§](#æ¨¡å—-8é«˜çº§ç‰¹æ€§)

---

# æ¨¡å— 1ï¼šé¡¹ç›®æ¶æ„ä¸æ ¸å¿ƒç†å¿µ

## 1.1 Monorepo ç»“æ„

```
/home/user/langchain/libs/
â”œâ”€ core/              â­ æ ¸å¿ƒæŠ½è±¡å±‚ï¼ˆæœ€é‡è¦ï¼‰
â”œâ”€ langchain_v1/      ğŸ“¦ ä¸»åŒ…
â”œâ”€ partners/          ğŸ”Œ å®˜æ–¹é›†æˆ
â””â”€ text-splitters/    âœ‚ï¸  å·¥å…·åº“
```

## 1.2 ä¸‰å¤§è®¾è®¡åŸåˆ™

### åŸåˆ™ 1ï¼šä¸€åˆ‡çš† Runnable
```python
class Runnable(ABC, Generic[Input, Output]):
    def invoke(self, input: Input) -> Output
    def batch(self, inputs: list[Input]) -> list[Output]
    def stream(self, input: Input) -> Iterator[Output]
    async def ainvoke(self, input: Input) -> Output
```

### åŸåˆ™ 2ï¼šå£°æ˜å¼ç»„åˆï¼ˆPipe `|`ï¼‰
```python
chain = component1 | component2 | component3
chain.invoke(input)  # è‡ªåŠ¨ä¸²è”æ‰§è¡Œ
```

### åŸåˆ™ 3ï¼šæ’ä»¶å¼é›†æˆ
```
langchain_core å®šä¹‰æ¥å£
    â†“
langchain_anthropic/openai å®ç°æ¥å£
```

## ğŸ§  çŸ¥è¯†æŒ‘æˆ˜

1. å“ªä¸ªåŒ…æ˜¯"åœ°åŸº"ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. Runnable çš„ 4 ä¸ªæ ¸å¿ƒæ–¹æ³•ï¼Ÿ
3. `prompt | llm` æ‰§è¡Œæ—¶å‘ç”Ÿä»€ä¹ˆï¼Ÿ

---

# æ¨¡å— 2ï¼šæ¶ˆæ¯ç³»ç»Ÿ (Messages)

## ğŸ“ æ¨¡å— 1 ç­”æ¡ˆ

1. **åœ°åŸºï¼š** `libs/core/` - æ‰€æœ‰åŒ…ä¾èµ–å®ƒï¼Œå®šä¹‰ç¨³å®šæ¥å£
2. **4ä¸ªæ–¹æ³•ï¼š** `invoke()` (å•æ¬¡), `batch()` (æ‰¹é‡), `stream()` (æµå¼), `ainvoke()` (å¼‚æ­¥)
3. **æ‰§è¡Œæµç¨‹ï¼š**
   ```
   ç”¨æˆ·è¾“å…¥ â†’ prompt.invoke() â†’ Promptå¯¹è±¡
           â†’ llm.invoke() â†’ AIMessage
   ```

## 2.1 æ¶ˆæ¯ç±»å‹å±‚æ¬¡

```
BaseMessage
â”œâ”€ HumanMessage (type="human")    # ç”¨æˆ·è¾“å…¥
â”œâ”€ AIMessage (type="ai")          # AIå›å¤
â”œâ”€ SystemMessage (type="system")  # ç³»ç»ŸæŒ‡ä»¤
â””â”€ ToolMessage (type="tool")      # å·¥å…·ç»“æœ
```

**æ–‡ä»¶ä½ç½®ï¼š**
- `libs/core/langchain_core/messages/base.py:93`
- `libs/core/langchain_core/messages/human.py:9`
- `libs/core/langchain_core/messages/ai.py`

## 2.2 æ ¸å¿ƒç”¨æ³•

```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# å…¸å‹å¯¹è¯ç»“æ„
conversation = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is 2+2?"),
    AIMessage(content="2+2 equals 4."),
]

# å¤šæ¨¡æ€å†…å®¹
msg = HumanMessage(content=[
    {"type": "text", "text": "What's in this image?"},
    {"type": "image_url", "image_url": {"url": "..."}}
])

# AI æ¶ˆæ¯å¸¦å·¥å…·è°ƒç”¨
ai_msg = AIMessage(
    content="Let me check the weather.",
    tool_calls=[{"name": "get_weather", "args": {"city": "SF"}}]
)
```

## ğŸ§  çŸ¥è¯†æŒ‘æˆ˜

1. ä¸‰ç§æ ¸å¿ƒæ¶ˆæ¯ç±»å‹åŠå…¶ç”¨é€”ï¼Ÿ
2. å¦‚ä½•æ„å»ºå®¢æœæœºå™¨äººçš„æç¤ºï¼Ÿï¼ˆä¼ªä»£ç ï¼‰
3. AIMessage çš„ `tool_calls` å­—æ®µä½œç”¨ï¼Ÿ

---

# æ¨¡å— 3ï¼šæç¤ºå·¥ç¨‹ (Prompts)

## ğŸ“ æ¨¡å— 2 ç­”æ¡ˆ

1. **ä¸‰ç§ç±»å‹ï¼š**
   - `SystemMessage` - è®¾å®šAIè¡Œä¸º
   - `HumanMessage` - ç”¨æˆ·è¾“å…¥
   - `AIMessage` - AIå›å¤

2. **å®¢æœæœºå™¨äººï¼š**
```python
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸“ä¸šå®¢æœï¼Œè§„åˆ™ï¼š1)ä¿æŒç¤¼è²Œ 2)ä¸çŸ¥é“å°±è½¬äººå·¥"),
    HumanMessage(content="æˆ‘çš„è®¢å•åœ¨å“ªï¼Ÿ")
]
```

3. **tool_callsï¼š** è®©AIè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆæœç´¢ã€è®¡ç®—å™¨ç­‰ï¼‰ï¼Œçªç ´LLMçš„çŸ¥è¯†é™åˆ¶

## 3.1 Prompt æ¨¡æ¿ç±»å‹

```
BasePromptTemplate
â”œâ”€ PromptTemplate        # å­—ç¬¦ä¸²æ¨¡æ¿
â””â”€ ChatPromptTemplate    # èŠå¤©æ¨¡æ¿
   â””â”€ MessagesPlaceholder # å†å²å ä½ç¬¦
```

**æ–‡ä»¶ä½ç½®ï¼š**
- `libs/core/langchain_core/prompts/prompt.py:24`
- `libs/core/langchain_core/prompts/chat.py`

## 3.2 æ ¸å¿ƒç”¨æ³•

### PromptTemplateï¼ˆç®€å•å­—ç¬¦ä¸²ï¼‰
```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    "Tell me a {adjective} joke about {topic}."
)
prompt.format(adjective="funny", topic="cats")
# â†’ "Tell me a funny joke about cats."

# ä½œä¸º Runnable
prompt.invoke({"adjective": "funny", "topic": "cats"})
# â†’ StringPromptValue(...)
```

### ChatPromptTemplateï¼ˆèŠå¤©æ¨¡æ¿ï¼‰
```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are {role}."),
    ("human", "{input}"),
])

prompt.invoke({"role": "chef", "input": "How to cook?"})
# â†’ ChatPromptValue(messages=[
#     SystemMessage(content="You are chef."),
#     HumanMessage(content="How to cook?"),
# ])
```

### MessagesPlaceholderï¼ˆå†å²å¯¹è¯ï¼‰
```python
from langchain_core.prompts import MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are helpful."),
    MessagesPlaceholder("history"),  # æ’å…¥å†å²
    ("human", "{question}"),
])

prompt.invoke({
    "history": [("human", "5+2?"), ("ai", "7")],
    "question": "multiply by 4"
})
# â†’ è‡ªåŠ¨å±•å¼€ history åˆ°å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
```

## 3.3 å®æˆ˜ï¼šç»„åˆ Prompt + Model

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a comedian."),
    ("human", "Joke about {topic}"),
])

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

chain = prompt | model  # LCEL ç»„åˆ

chain.invoke({"topic": "programming"})
# â†’ AIMessage("Why do programmers prefer dark mode?...")
```

**æ‰§è¡Œæµç¨‹ï¼š**
```
{"topic": "programming"}
    â†“ prompt.invoke()
ChatPromptValue([SystemMessage(...), HumanMessage(...)])
    â†“ model.invoke()
AIMessage("...")
```

## ğŸ§  çŸ¥è¯†æŒ‘æˆ˜

1. `PromptTemplate` vs `ChatPromptTemplate` åŒºåˆ«ï¼Ÿ
2. `MessagesPlaceholder` è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ
3. é¢„æµ‹è¾“å‡ºï¼š`(prompt | model).invoke({"role": "chef", "input": "pasta?"})`

---

# æ¨¡å— 4ï¼šæ ¸å¿ƒæŠ½è±¡ Runnable

## ğŸ“ æ¨¡å— 3 ç­”æ¡ˆ

1. **åŒºåˆ«ï¼š**
   - `PromptTemplate` â†’ ç”Ÿæˆå­—ç¬¦ä¸² (StringPromptValue)
   - `ChatPromptTemplate` â†’ ç”Ÿæˆæ¶ˆæ¯åˆ—è¡¨ (ChatPromptValue)

2. **MessagesPlaceholderï¼š** åŠ¨æ€æ’å…¥å¯¹è¯å†å²ï¼Œé¿å…æ‰‹åŠ¨æ‹¼æ¥æ¶ˆæ¯åˆ—è¡¨

3. **é¢„æµ‹è¾“å‡ºï¼š**
```python
# æ­¥éª¤ 1: prompt.invoke() ç”Ÿæˆæ¶ˆæ¯
# æ­¥éª¤ 2: model.invoke() è°ƒç”¨ Claude
# æœ€ç»ˆè¾“å‡º: AIMessage(content="To make pasta, first boil water...")
```

## 4.1 Runnable åè®®æ·±åº¦è§£æ

**æ–‡ä»¶ä½ç½®ï¼š** `libs/core/langchain_core/runnables/base.py:124`

```python
class Runnable(ABC, Generic[Input, Output]):
    """å¯è°ƒç”¨ã€æ‰¹å¤„ç†ã€æµå¼å¤„ç†ã€è½¬æ¢å’Œç»„åˆçš„å·¥ä½œå•å…ƒ"""

    # ğŸ”‘ æ ¸å¿ƒæ–¹æ³•ï¼ˆå¿…é¡»å®ç°ï¼‰
    @abstractmethod
    def invoke(self, input: Input, config: RunnableConfig | None = None) -> Output:
        """å•æ¬¡åŒæ­¥è°ƒç”¨"""

    # ğŸ”‘ é»˜è®¤å®ç°ï¼ˆå¯é€‰é‡å†™ä¼˜åŒ–ï¼‰
    def batch(
        self,
        inputs: list[Input],
        config: RunnableConfig | None = None
    ) -> list[Output]:
        """æ‰¹é‡å¤„ç†ï¼ˆé»˜è®¤å¹¶è¡Œè°ƒç”¨ invokeï¼‰"""

    def stream(
        self,
        input: Input,
        config: RunnableConfig | None = None
    ) -> Iterator[Output]:
        """æµå¼è¾“å‡ºï¼ˆé»˜è®¤ä¸€æ¬¡æ€§è¿”å›ï¼‰"""

    async def ainvoke(
        self,
        input: Input,
        config: RunnableConfig | None = None
    ) -> Output:
        """å¼‚æ­¥è°ƒç”¨ï¼ˆé»˜è®¤åœ¨çº¿ç¨‹æ± æ‰§è¡Œ invokeï¼‰"""
```

## 4.2 ä¸ºä»€ä¹ˆ Runnable å¦‚æ­¤é‡è¦ï¼Ÿ

### åŸå›  1ï¼šç»Ÿä¸€æ¥å£
æ‰€æœ‰ç»„ä»¶éƒ½å®ç° Runnable â†’ ä¸€è‡´çš„è°ƒç”¨æ–¹å¼

```python
# è¿™äº›éƒ½æœ‰ç›¸åŒçš„æ–¹æ³•ï¼
prompt.invoke(...)
model.invoke(...)
chain.invoke(...)
retriever.invoke(...)
```

### åŸå›  2ï¼šè‡ªåŠ¨ç»„åˆ
ä½¿ç”¨ `|` åˆ›å»ºçš„é“¾ä¼š**è‡ªåŠ¨ç»§æ‰¿**æ‰€æœ‰ Runnable æ–¹æ³•

```python
chain = prompt | model | output_parser

# è‡ªåŠ¨æ”¯æŒï¼š
chain.invoke(input)
chain.batch([input1, input2])
chain.stream(input)
await chain.ainvoke(input)
```

### åŸå›  3ï¼šé…ç½®ä¼ é€’
`RunnableConfig` è‡ªåŠ¨åœ¨é“¾ä¸­ä¼ é€’ï¼ˆç”¨äºå›è°ƒã€æ ‡ç­¾ã€å…ƒæ•°æ®ç­‰ï¼‰

```python
chain.invoke(
    input,
    config={
        "tags": ["experiment"],
        "metadata": {"user_id": "123"}
    }
)
# config ä¼šä¼ é€’ç»™é“¾ä¸­çš„æ¯ä¸ªç»„ä»¶
```

## 4.3 Runnable ç»„åˆåŸè¯­

### RunnableSequenceï¼ˆé¡ºåºæ‰§è¡Œï¼‰
```python
from langchain_core.runnables import RunnableLambda

# ä½¿ç”¨ | åˆ›å»ºï¼ˆæ¨èï¼‰
seq = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)
seq.invoke(5)  # (5+1)*2 = 12

# ç­‰ä»·äº
from langchain_core.runnables import RunnableSequence
seq = RunnableSequence(steps=[...])
```

**æ‰§è¡Œæµç¨‹ï¼š**
```
Input(5) â†’ [step1: +1] â†’ 6 â†’ [step2: *2] â†’ Output(12)
```

### RunnableParallelï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰
```python
from langchain_core.runnables import RunnableParallel

parallel = RunnableParallel(
    add=RunnableLambda(lambda x: x + 1),
    mul=RunnableLambda(lambda x: x * 2),
)
parallel.invoke(5)
# {"add": 6, "mul": 10}

# åœ¨é“¾ä¸­ä½¿ç”¨
chain = parallel | RunnableLambda(lambda d: d["add"] + d["mul"])
chain.invoke(5)  # 6 + 10 = 16
```

**æ‰§è¡Œæµç¨‹ï¼š**
```
Input(5) â”¬â†’ [add: +1] â†’ 6  â”
         â””â†’ [mul: *2] â†’ 10 â”˜â†’ {"add": 6, "mul": 10}
```

### RunnableLambdaï¼ˆè‡ªå®šä¹‰å‡½æ•°ï¼‰
```python
from langchain_core.runnables import RunnableLambda

def my_function(input_dict):
    return input_dict["x"] * 2

runnable_fn = RunnableLambda(my_function)
runnable_fn.invoke({"x": 5})  # 10

# æˆ–ä½¿ç”¨è£…é¥°å™¨
@RunnableLambda
def my_function(x):
    return x * 2
```

## 4.4 å®æˆ˜ï¼šæ„å»ºå¤æ‚é“¾

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_anthropic import ChatAnthropic

# æ­¥éª¤ 1: åˆ›å»ºä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯
branch_a = ChatPromptTemplate.from_template("Summarize: {text}") | model
branch_b = ChatPromptTemplate.from_template("Translate to French: {text}") | model

# æ­¥éª¤ 2: å¹¶è¡Œæ‰§è¡Œ
parallel_chain = RunnableParallel(summary=branch_a, translation=branch_b)

# æ­¥éª¤ 3: åˆå¹¶ç»“æœ
def combine(results):
    return f"Summary: {results['summary'].content}\n\nFrench: {results['translation'].content}"

final_chain = parallel_chain | RunnableLambda(combine)

# æ‰§è¡Œ
final_chain.invoke({"text": "LangChain is awesome!"})
```

**æµç¨‹å›¾ï¼š**
```
{"text": "..."}
    â”Œâ†’ [Summarize prompt] â†’ [Model] â†’ summary â”
    â””â†’ [Translate prompt] â†’ [Model] â†’ translation â”˜
                    â†“
              [Combine function]
                    â†“
        "Summary: ...\nFrench: ..."
```

## ğŸ§  çŸ¥è¯†æŒ‘æˆ˜

1. Runnable çš„ 4 ä¸ªæ ¸å¿ƒæ–¹æ³•ï¼Œå“ªä¸ªæ˜¯æŠ½è±¡æ–¹æ³•ï¼ˆå¿…é¡»å®ç°ï¼‰ï¼Ÿ
2. `RunnableSequence` å’Œ `RunnableParallel` çš„åŒºåˆ«ï¼Ÿä¸¾ä¾‹è¯´æ˜
3. é¢„æµ‹è¾“å‡ºï¼š
```python
chain = RunnableLambda(lambda x: x["a"] + x["b"]) | RunnableLambda(lambda x: x * 3)
chain.invoke({"a": 2, "b": 3})
```

---

# æ¨¡å— 5ï¼šèŠå¤©æ¨¡å‹ (Chat Models)

## ğŸ“ æ¨¡å— 4 ç­”æ¡ˆ

1. **æŠ½è±¡æ–¹æ³•ï¼š** `invoke()` æ˜¯å”¯ä¸€çš„æŠ½è±¡æ–¹æ³•ï¼Œå…¶ä»–æ–¹æ³•æœ‰é»˜è®¤å®ç°

2. **åŒºåˆ«ï¼š**
   - `RunnableSequence` - é¡ºåºæ‰§è¡Œï¼Œè¾“å‡ºâ†’è¾“å…¥é“¾å¼ä¼ é€’
   - `RunnableParallel` - å¹¶è¡Œæ‰§è¡Œï¼Œæ‰€æœ‰åˆ†æ”¯æ¥æ”¶ç›¸åŒè¾“å…¥ï¼Œè¿”å›å­—å…¸

3. **é¢„æµ‹è¾“å‡ºï¼š**
```python
# æ­¥éª¤1: lambda x: x["a"] + x["b"] â†’ 2 + 3 = 5
# æ­¥éª¤2: lambda x: x * 3 â†’ 5 * 3 = 15
# è¾“å‡º: 15
```

## 5.1 BaseChatModel æ¶æ„

**æ–‡ä»¶ä½ç½®ï¼š** `libs/core/langchain_core/language_models/chat_models.py`

```python
class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):
    """èŠå¤©æ¨¡å‹çš„æŠ½è±¡åŸºç±»

    æ‰€æœ‰èŠå¤©æ¨¡å‹æä¾›å•†ï¼ˆOpenAIã€Anthropicç­‰ï¼‰éƒ½ç»§æ‰¿è¿™ä¸ªç±»
    """

    @abstractmethod
    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        """æ ¸å¿ƒç”Ÿæˆæ–¹æ³•ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""

    # é«˜çº§ç‰¹æ€§
    def bind_tools(
        self,
        tools: Sequence[BaseTool | dict],
        **kwargs: Any,
    ) -> Runnable[LanguageModelInput, BaseMessage]:
        """ç»‘å®šå·¥å…·ä»¥æ”¯æŒå‡½æ•°è°ƒç”¨"""

    def with_structured_output(
        self,
        schema: type[BaseModel] | dict,
        **kwargs: Any,
    ) -> Runnable[LanguageModelInput, BaseModel | dict]:
        """å¼ºåˆ¶è¾“å‡ºç¬¦åˆç‰¹å®šç»“æ„"""
```

## 5.2 ä½¿ç”¨èŠå¤©æ¨¡å‹

### åŸºæœ¬è°ƒç”¨
```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.7,
    max_tokens=1024,
)

# æ–¹å¼ 1: ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is the capital of France?"),
]
response = model.invoke(messages)
# â†’ AIMessage(content="The capital of France is Paris.")

# æ–¹å¼ 2: ä½¿ç”¨å­—ç¬¦ä¸²ï¼ˆè‡ªåŠ¨è½¬ä¸º HumanMessageï¼‰
response = model.invoke("What is 2+2?")
# â†’ AIMessage(content="2+2 equals 4.")
```

### æµå¼è¾“å‡º
```python
for chunk in model.stream("Tell me a long story"):
    print(chunk.content, end="", flush=True)
# è¾“å‡º: Once... upon... a... time...
```

### æ‰¹é‡å¤„ç†
```python
results = model.batch([
    "What is 1+1?",
    "What is 2+2?",
    "What is 3+3?",
])
# â†’ [AIMessage("2"), AIMessage("4"), AIMessage("6")]
```

## 5.3 å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰

```python
from langchain_core.tools import tool

# å®šä¹‰å·¥å…·
@tool
def get_weather(location: str) -> str:
    """Get the weather for a location."""
    return f"The weather in {location} is sunny."

# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
model_with_tools = model.bind_tools([get_weather])

# AI ä¼šç”Ÿæˆå·¥å…·è°ƒç”¨
response = model_with_tools.invoke("What's the weather in SF?")
print(response.tool_calls)
# â†’ [{"name": "get_weather", "args": {"location": "SF"}, "id": "..."}]

# æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ
from langchain_core.messages import ToolMessage

tool_call = response.tool_calls[0]
tool_result = get_weather.invoke(tool_call["args"])

messages = [
    HumanMessage(content="What's the weather in SF?"),
    response,  # AIMessage with tool_calls
    ToolMessage(content=tool_result, tool_call_id=tool_call["id"]),
]

final_response = model.invoke(messages)
# â†’ AIMessage("The weather in San Francisco is sunny.")
```

**å·¥å…·è°ƒç”¨æµç¨‹ï¼š**
```
ç”¨æˆ·é—®é¢˜ â†’ AIç”Ÿæˆtool_call â†’ æ‰§è¡Œå·¥å…· â†’ è¿”å›ToolMessage â†’ AIç”Ÿæˆæœ€ç»ˆå›ç­”
```

## 5.4 ç»“æ„åŒ–è¾“å‡º

```python
from pydantic import BaseModel, Field

class Joke(BaseModel):
    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline of the joke")

structured_model = model.with_structured_output(Joke)

result = structured_model.invoke("Tell me a joke about cats")
# â†’ Joke(
#     setup="Why don't cats play poker in the jungle?",
#     punchline="Too many cheetahs!"
# )

print(result.setup)      # ç±»å‹å®‰å…¨çš„è®¿é—®
print(result.punchline)
```

## ğŸ§  çŸ¥è¯†æŒ‘æˆ˜

1. `BaseChatModel` çš„æ ¸å¿ƒæŠ½è±¡æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ
2. `bind_tools()` å’Œ `with_structured_output()` çš„åŒºåˆ«ï¼Ÿ
3. å·¥å…·è°ƒç”¨çš„å®Œæ•´æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆç”¨æµç¨‹å›¾æè¿°ï¼‰

---

# æ¨¡å— 6ï¼šé“¾å¼ç»„åˆ (Chains & LCEL)

## ğŸ“ æ¨¡å— 5 ç­”æ¡ˆ

1. **æ ¸å¿ƒæŠ½è±¡æ–¹æ³•ï¼š** `_generate()` - å­ç±»å¿…é¡»å®ç°è¿™ä¸ªæ–¹æ³•æ¥ç”Ÿæˆå“åº”

2. **åŒºåˆ«ï¼š**
   - `bind_tools()` - è®©AIèƒ½è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆAIå†³å®šæ˜¯å¦è°ƒç”¨ï¼‰
   - `with_structured_output()` - å¼ºåˆ¶AIè¾“å‡ºç¬¦åˆç‰¹å®šç»“æ„ï¼ˆPydanticæ¨¡å‹ï¼‰

3. **å·¥å…·è°ƒç”¨æµç¨‹ï¼š**
```
ç”¨æˆ·é—®é¢˜
  â†’ AIåˆ†æå¹¶ç”Ÿæˆ tool_call
  â†’ ä½ çš„ä»£ç æ‰§è¡Œå·¥å…·å‡½æ•°
  â†’ å°†ç»“æœä½œä¸º ToolMessage è¿”å›
  â†’ AI åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
```

## 6.1 ä»€ä¹ˆæ˜¯ LCELï¼Ÿ

**LCEL = LangChain Expression Language**

è¿™æ˜¯ LangChain çš„"ç®¡é“è¯­æ³•"ï¼Œç”¨äºä»¥å£°æ˜å¼æ–¹å¼ç»„åˆç»„ä»¶ã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š** ä½¿ç”¨ `|` æ“ä½œç¬¦è¿æ¥ Runnable å¯¹è±¡

```python
# ä¼ ç»Ÿæ–¹å¼ï¼ˆå‘½ä»¤å¼ï¼‰
prompt_result = prompt.invoke(input)
model_result = model.invoke(prompt_result)
parser_result = parser.invoke(model_result)

# LCEL æ–¹å¼ï¼ˆå£°æ˜å¼ï¼‰
chain = prompt | model | parser
result = chain.invoke(input)
```

**ä¸ºä»€ä¹ˆ LCEL æ›´å¥½ï¼Ÿ**
- âœ… **è‡ªåŠ¨æ”¯æŒæ‰€æœ‰ Runnable æ–¹æ³•**ï¼ˆinvokeã€batchã€streamã€ainvokeï¼‰
- âœ… **å¯ç»„åˆ** - é“¾å¯ä»¥ä½œä¸ºå…¶ä»–é“¾çš„ç»„ä»¶
- âœ… **è‡ªåŠ¨é”™è¯¯å¤„ç†å’Œé‡è¯•**
- âœ… **å†…ç½®è¿½è¸ªå’Œè°ƒè¯•**

## 6.2 LCEL æ ¸å¿ƒæ“ä½œç¬¦

### 1ï¸âƒ£ ç®¡é“æ“ä½œç¬¦ `|`ï¼ˆé¡ºåºç»„åˆï¼‰

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
parser = StrOutputParser()  # æå– AIMessage.content

chain = prompt | model | parser

result = chain.invoke({"topic": "cats"})
# â†’ "Why did the cat sit on the computer? ..." (ç›´æ¥æ˜¯å­—ç¬¦ä¸²)
```

**æ•°æ®æµï¼š**
```
{"topic": "cats"}
  â†’ prompt â†’ ChatPromptValue([HumanMessage("Tell me a joke about cats")])
  â†’ model  â†’ AIMessage(content="Why did the cat...")
  â†’ parser â†’ "Why did the cat..." (str)
```

### 2ï¸âƒ£ å­—å…¸æ“ä½œç¬¦ `{}`ï¼ˆå¹¶è¡Œç»„åˆï¼‰

```python
from langchain_core.runnables import RunnablePassthrough

chain = {
    "context": retriever,  # å¹¶è¡Œåˆ†æ”¯ 1
    "question": RunnablePassthrough()  # å¹¶è¡Œåˆ†æ”¯ 2ï¼ˆç›´æ¥ä¼ é€’è¾“å…¥ï¼‰
} | prompt | model

chain.invoke("What is LangChain?")
```

**æ•°æ®æµï¼š**
```
"What is LangChain?"
    â”Œâ†’ retriever.invoke() â†’ æ£€ç´¢åˆ°çš„æ–‡æ¡£ â”
    â””â†’ RunnablePassthrough() â†’ "What is LangChain?" â”˜
              â†“
    {"context": "...", "question": "What is LangChain?"}
              â†“
          prompt | model
```

### 3ï¸âƒ£ æ¡ä»¶åˆ†æ”¯ `RunnableBranch`

```python
from langchain_core.runnables import RunnableBranch, RunnableLambda

def is_short(input):
    return len(input) < 10

branch = RunnableBranch(
    (is_short, RunnableLambda(lambda x: f"Short: {x}")),
    (lambda x: len(x) < 50, RunnableLambda(lambda x: f"Medium: {x}")),
    RunnableLambda(lambda x: f"Long: {x}")  # é»˜è®¤åˆ†æ”¯
)

branch.invoke("Hi")        # â†’ "Short: Hi"
branch.invoke("Hello world!")  # â†’ "Medium: Hello world!"
```

## 6.3 å®æˆ˜ï¼šRAG é“¾ï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# å‡è®¾ä½ æœ‰ä¸€ä¸ªå‘é‡æ•°æ®åº“æ£€ç´¢å™¨
# retriever = ...

prompt = ChatPromptTemplate.from_template("""
Answer the question based on the context:

Context: {context}

Question: {question}
""")

# æ„å»º RAG é“¾
rag_chain = (
    {
        "context": lambda x: retriever.invoke(x["question"]),
        "question": lambda x: x["question"]
    }
    | prompt
    | model
    | StrOutputParser()
)

result = rag_chain.invoke({"question": "What is LangChain?"})
```

**æµç¨‹å›¾ï¼š**
```
{"question": "What is LangChain?"}
    â”Œâ†’ retriever â†’ "LangChain is a framework..." (context) â”
    â””â†’ passthrough â†’ "What is LangChain?" (question) â”˜
                â†“
    {"context": "...", "question": "..."}
                â†“
            prompt â†’ ChatPromptValue
                â†“
            model â†’ AIMessage
                â†“
            parser â†’ str
```

## 6.4 LCEL é«˜çº§ç‰¹æ€§

### Fallbacksï¼ˆæ•…éšœè½¬ç§»ï¼‰
```python
primary_model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
backup_model = ChatOpenAI(model="gpt-4")

chain = primary_model.with_fallbacks([backup_model])

# å¦‚æœ primary_model å¤±è´¥ï¼Œè‡ªåŠ¨ä½¿ç”¨ backup_model
result = chain.invoke("Hello")
```

### Retryï¼ˆé‡è¯•ï¼‰
```python
chain = (prompt | model).with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
)
```

### Configurable Alternativesï¼ˆå¯é…ç½®æ›¿ä»£ï¼‰
```python
from langchain_core.runnables import ConfigurableField

model = ChatAnthropic(model="claude-3-5-sonnet-20241022").configurable_fields(
    model=ConfigurableField(id="model_name")
)

chain = prompt | model

# è¿è¡Œæ—¶åˆ‡æ¢æ¨¡å‹
chain.invoke(
    {"topic": "cats"},
    config={"configurable": {"model_name": "claude-3-opus-20240229"}}
)
```

## ğŸ§  çŸ¥è¯†æŒ‘æˆ˜

1. LCEL çš„ `|` æ“ä½œç¬¦å’Œ Python å­—å…¸ `{}` åœ¨é“¾ä¸­åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆï¼Ÿ
2. ä¸ºä»€ä¹ˆ `StrOutputParser` æœ‰ç”¨ï¼Ÿä¸ç”¨å®ƒä¼šæ€æ ·ï¼Ÿ
3. è®¾è®¡ä¸€ä¸ª LCEL é“¾ï¼Œå®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š
   - è¾“å…¥ç”¨æˆ·é—®é¢˜
   - å¹¶è¡Œæ‰§è¡Œï¼šç¿»è¯‘æˆæ³•è¯­ + ç”Ÿæˆå›ç­”
   - åˆå¹¶ç»“æœè¾“å‡º

---

# æ¨¡å— 7ï¼šå·¥å…·ä¸ä»£ç† (Tools & Agents)

## ğŸ“ æ¨¡å— 6 ç­”æ¡ˆ

1. **æ“ä½œç¬¦å«ä¹‰ï¼š**
   - `|` - é¡ºåºæ‰§è¡Œï¼ˆç®¡é“ï¼‰ï¼Œè¾“å‡ºä¼ é€’ç»™ä¸‹ä¸€ä¸ªç»„ä»¶
   - `{}` - å¹¶è¡Œæ‰§è¡Œï¼Œç›¸åŒè¾“å…¥åˆ°å¤šä¸ªåˆ†æ”¯ï¼Œè¾“å‡ºåˆå¹¶ä¸ºå­—å…¸

2. **StrOutputParserï¼š**
   - ä½œç”¨ï¼šæå– `AIMessage.content` ä¸ºçº¯å­—ç¬¦ä¸²
   - ä¸ç”¨å®ƒï¼š`invoke()` è¿”å›å®Œæ•´çš„ `AIMessage` å¯¹è±¡

3. **è®¾è®¡æ–¹æ¡ˆï¼š**
```python
chain = (
    {
        "translation": translate_prompt | model,
        "answer": answer_prompt | model
    }
    | RunnableLambda(lambda d: f"Answer: {d['answer'].content}\nFrench: {d['translation'].content}")
)
```

## 7.1 ä»€ä¹ˆæ˜¯ Toolï¼Ÿ

**Tool = å¯ä¾› AI è°ƒç”¨çš„ Python å‡½æ•°**

**æ–‡ä»¶ä½ç½®ï¼š** `libs/core/langchain_core/tools/base.py`

```python
from langchain_core.tools import BaseTool

class BaseTool(ABC, RunnableSerializable[str, Any]):
    """å·¥å…·çš„æŠ½è±¡åŸºç±»

    å·¥å…·è®© AI èƒ½å¤Ÿæ‰§è¡Œå¤–éƒ¨æ“ä½œï¼ˆæœç´¢ã€è®¡ç®—ã€APIè°ƒç”¨ç­‰ï¼‰
    """

    name: str
    """å·¥å…·çš„å”¯ä¸€åç§°"""

    description: str
    """å·¥å…·åŠŸèƒ½æè¿°ï¼ˆAI é€šè¿‡è¿™ä¸ªå†³å®šä½•æ—¶è°ƒç”¨ï¼‰"""

    @abstractmethod
    def _run(self, *args: Any, **kwargs: Any) -> Any:
        """å·¥å…·çš„æ‰§è¡Œé€»è¾‘"""
```

## 7.2 åˆ›å»ºå·¥å…·çš„ä¸‰ç§æ–¹å¼

### æ–¹å¼ 1ï¼šä½¿ç”¨ @tool è£…é¥°å™¨ï¼ˆæ¨èï¼‰

```python
from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers.

    Args:
        a: First number
        b: Second number
    """
    return a * b

# è‡ªåŠ¨ç”Ÿæˆçš„å±æ€§
print(multiply.name)         # "multiply"
print(multiply.description)  # "Multiply two numbers..."
print(multiply.args_schema)  # Pydantic æ¨¡å‹ï¼ˆä»ç±»å‹æ³¨è§£ç”Ÿæˆï¼‰

# è°ƒç”¨
result = multiply.invoke({"a": 3, "b": 4})  # 12
```

### æ–¹å¼ 2ï¼šä½¿ç”¨ StructuredTool

```python
from langchain_core.tools import StructuredTool
from pydantic import BaseModel, Field

class SearchInput(BaseModel):
    query: str = Field(description="The search query")
    num_results: int = Field(default=5, description="Number of results")

def search_function(query: str, num_results: int = 5) -> str:
    return f"Found {num_results} results for '{query}'"

search_tool = StructuredTool.from_function(
    func=search_function,
    name="web_search",
    description="Search the web for information",
    args_schema=SearchInput
)
```

### æ–¹å¼ 3ï¼šç»§æ‰¿ BaseTool

```python
from langchain_core.tools import BaseTool
from typing import Type

class CustomTool(BaseTool):
    name: str = "custom_calculator"
    description: str = "Performs advanced calculations"

    def _run(self, expression: str) -> str:
        try:
            return str(eval(expression))  # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒä¸è¦ç”¨ evalï¼
        except Exception as e:
            return f"Error: {str(e)}"

    async def _arun(self, expression: str) -> str:
        # å¼‚æ­¥ç‰ˆæœ¬
        return self._run(expression)

tool = CustomTool()
tool.invoke({"expression": "2 + 2"})  # "4"
```

## 7.3 Agent æ¶æ„

**Agent = èƒ½å¤Ÿä½¿ç”¨å·¥å…·çš„è‡ªä¸» AI ç³»ç»Ÿ**

**æ ¸å¿ƒæ¦‚å¿µï¼š**
1. AI æ¥æ”¶ä»»åŠ¡
2. AI **å†³å®š**éœ€è¦è°ƒç”¨å“ªäº›å·¥å…·
3. æ‰§è¡Œå·¥å…·
4. AI æ ¹æ®ç»“æœç»§ç»­æ¨ç†
5. å¾ªç¯ç›´åˆ°ä»»åŠ¡å®Œæˆ

### ç®€å• Agent å®ç°ï¼ˆä½¿ç”¨ LCELï¼‰

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain_anthropic import ChatAnthropic

# å®šä¹‰å·¥å…·
@tool
def get_word_length(word: str) -> int:
    """Returns the length of a word."""
    return len(word)

@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

tools = [get_word_length, multiply]

# åˆ›å»ºæ”¯æŒå·¥å…·çš„æ¨¡å‹
model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
model_with_tools = model.bind_tools(tools)

# Agent æç¤º
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Use tools when necessary."),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder("agent_scratchpad"),  # å·¥å…·è°ƒç”¨è®°å½•
])

# Agent å¾ªç¯é€»è¾‘
from langchain_core.messages import AIMessage, ToolMessage

def run_agent(user_input: str, max_iterations: int = 5):
    messages = [("human", user_input)]

    for i in range(max_iterations):
        # AI ç”Ÿæˆå“åº”ï¼ˆå¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨ï¼‰
        response = model_with_tools.invoke(messages)
        messages.append(response)

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if not response.tool_calls:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä»»åŠ¡å®Œæˆ
            return response.content

        # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        for tool_call in response.tool_calls:
            tool = next(t for t in tools if t.name == tool_call["name"])
            tool_result = tool.invoke(tool_call["args"])

            # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯
            messages.append(ToolMessage(
                content=str(tool_result),
                tool_call_id=tool_call["id"]
            ))

    return "Max iterations reached"

# æµ‹è¯•
result = run_agent("What is the length of 'hello' multiplied by 3?")
print(result)
```

**Agent æ‰§è¡Œæµç¨‹ï¼š**
```
ç”¨æˆ·: "What is the length of 'hello' multiplied by 3?"
    â†“
AI: "Let me check the length first"
    â†’ tool_call: get_word_length("hello")
    â†“
ToolMessage: "5"
    â†“
AI: "Now multiply 5 by 3"
    â†’ tool_call: multiply(5, 3)
    â†“
ToolMessage: "15"
    â†“
AI: "The length of 'hello' is 5, and 5 multiplied by 3 equals 15."
```

## 7.4 LangGraphï¼ˆé«˜çº§ Agent æ¡†æ¶ï¼‰

å¯¹äºå¤æ‚çš„ Agentï¼Œæ¨èä½¿ç”¨ **LangGraph**ï¼ˆLangChain ç”Ÿæ€ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ï¼‰ï¼š

```python
from langgraph.prebuilt import create_react_agent

# åˆ›å»º ReAct Agentï¼ˆæ¨ç†+è¡ŒåŠ¨å¾ªç¯ï¼‰
agent_executor = create_react_agent(
    model=model,
    tools=tools
)

# æµå¼æ‰§è¡Œ
for event in agent_executor.stream({"messages": [("human", "What is 2+2?")]}):
    print(event)
```

**LangGraph ä¼˜åŠ¿ï¼š**
- âœ… çŠ¶æ€ç®¡ç†ï¼ˆè®°å¿†ï¼‰
- âœ… äººæœºååŒï¼ˆHuman-in-the-loopï¼‰
- âœ… å¯è§†åŒ–å·¥ä½œæµ
- âœ… æŒä¹…åŒ–æ£€æŸ¥ç‚¹

## ğŸ§  çŸ¥è¯†æŒ‘æˆ˜

1. å·¥å…·çš„ä¸‰ä¸ªå¿…éœ€å±æ€§æ˜¯ä»€ä¹ˆï¼Ÿ
2. Agent å’Œæ™®é€š Chain çš„æœ¬è´¨åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
3. è®¾è®¡ä¸€ä¸ªå·¥å…·ï¼šæ£€æŸ¥ç»™å®šURLæ˜¯å¦å¯è®¿é—®ï¼ˆè¿”å›True/Falseï¼‰

---

# æ¨¡å— 8ï¼šé«˜çº§ç‰¹æ€§

## ğŸ“ æ¨¡å— 7 ç­”æ¡ˆ

1. **å·¥å…·ä¸‰è¦ç´ ï¼š**
   - `name` - å”¯ä¸€æ ‡è¯†ç¬¦
   - `description` - åŠŸèƒ½æè¿°ï¼ˆAI ç”¨äºåˆ¤æ–­ä½•æ—¶è°ƒç”¨ï¼‰
   - `_run()` æ–¹æ³• - å®é™…æ‰§è¡Œé€»è¾‘

2. **æœ¬è´¨åŒºåˆ«ï¼š**
   - **Chain** - é¢„å®šä¹‰çš„æ‰§è¡Œæµç¨‹ï¼Œå›ºå®šé¡ºåº
   - **Agent** - AI **åŠ¨æ€å†³å®š**ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼ŒåŒ…æ‹¬æ˜¯å¦/ä½•æ—¶è°ƒç”¨å·¥å…·

3. **URLæ£€æŸ¥å·¥å…·ï¼š**
```python
import requests
from langchain_core.tools import tool

@tool
def check_url(url: str) -> bool:
    """Check if a URL is accessible.

    Args:
        url: The URL to check

    Returns:
        True if accessible, False otherwise
    """
    try:
        response = requests.get(url, timeout=5)
        return response.status_code == 200
    except:
        return False
```

## 8.1 Callbacksï¼ˆå›è°ƒç³»ç»Ÿï¼‰

**ç”¨é€”ï¼š** è¿½è¸ªã€æ—¥å¿—ã€è°ƒè¯•ã€ç›‘æ§

**æ–‡ä»¶ä½ç½®ï¼š** `libs/core/langchain_core/callbacks/`

```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage

class MyCallbackHandler(BaseCallbackHandler):
    """è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨"""

    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"[LLM Start] Prompts: {prompts}")

    def on_llm_end(self, response, **kwargs):
        print(f"[LLM End] Response: {response}")

    def on_chain_start(self, serialized, inputs, **kwargs):
        print(f"[Chain Start] Inputs: {inputs}")

# ä½¿ç”¨å›è°ƒ
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

result = model.invoke(
    "Hello",
    config={"callbacks": [MyCallbackHandler()]}
)

# è¾“å‡º:
# [LLM Start] Prompts: ['Hello']
# [LLM End] Response: ...
```

### å†…ç½®å›è°ƒ

```python
from langchain.callbacks import StdOutCallbackHandler

# æ‰“å°æ‰€æœ‰ä¸­é—´æ­¥éª¤åˆ°æ ‡å‡†è¾“å‡º
handler = StdOutCallbackHandler()

chain = prompt | model
chain.invoke("Hello", config={"callbacks": [handler]})
```

## 8.2 Streamingï¼ˆæµå¼å¤„ç†ï¼‰

### æµå¼è¾“å‡ºæ–‡æœ¬

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# é€ token æµå¼è¾“å‡º
for chunk in model.stream("Tell me a long story"):
    print(chunk.content, end="", flush=True)
```

### æµå¼å¤„ç†é“¾ä¸­çš„ä¸­é—´æ­¥éª¤

```python
chain = prompt | model | StrOutputParser()

# ä½¿ç”¨ astream_events è·å–æ‰€æœ‰äº‹ä»¶
async for event in chain.astream_events("Tell me a joke", version="v2"):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event["data"]["chunk"].content, end="")
    elif kind == "on_parser_stream":
        print(f"[Parser]: {event['data']['chunk']}")
```

## 8.3 Memoryï¼ˆè®°å¿†ç³»ç»Ÿï¼‰

**æ–‡ä»¶ä½ç½®ï¼š** `libs/core/langchain_core/chat_history.py`

```python
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory

# å†…å­˜å­˜å‚¨
store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

# åˆ›å»ºå¸¦è®°å¿†çš„é“¾
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder("history"),
    ("human", "{input}"),
])

chain = prompt | model

chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

# ä½¿ç”¨ï¼ˆè‡ªåŠ¨ä¿å­˜å†å²ï¼‰
chain_with_history.invoke(
    {"input": "Hi, I'm Alice"},
    config={"configurable": {"session_id": "user-123"}}
)
# â†’ "Hello Alice! How can I help you?"

chain_with_history.invoke(
    {"input": "What's my name?"},
    config={"configurable": {"session_id": "user-123"}}
)
# â†’ "Your name is Alice!"
```

## 8.4 Cachingï¼ˆç¼“å­˜ï¼‰

**æ–‡ä»¶ä½ç½®ï¼š** `libs/core/langchain_core/caches.py`

```python
from langchain_core.caches import InMemoryCache
from langchain_core.globals import set_llm_cache

# å¯ç”¨å…¨å±€ç¼“å­˜
set_llm_cache(InMemoryCache())

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ…¢ï¼‰
result1 = model.invoke("What is 2+2?")

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆå¿«ï¼ä»ç¼“å­˜è¯»å–ï¼‰
result2 = model.invoke("What is 2+2?")  # ç›¸åŒè¾“å…¥ç›´æ¥è¿”å›ç¼“å­˜
```

## 8.5 Retrievalï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆ RAGï¼‰

```python
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.embeddings import Embeddings

# åˆ›å»ºå‘é‡æ•°æ®åº“
vectorstore = InMemoryVectorStore.from_texts(
    ["LangChain is a framework for LLMs", "Paris is the capital of France"],
    embedding=embeddings  # éœ€è¦åµŒå…¥æ¨¡å‹
)

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# RAG é“¾
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("""
Answer based on context:

Context: {context}

Question: {question}
""")

rag_chain = (
    {"context": retriever, "question": lambda x: x}
    | prompt
    | model
    | StrOutputParser()
)

rag_chain.invoke("What is LangChain?")
# â†’ "LangChain is a framework for LLMs..."
```

## 8.6 Output Parsersï¼ˆè¾“å‡ºè§£æå™¨ï¼‰

**æ–‡ä»¶ä½ç½®ï¼š** `libs/core/langchain_core/output_parsers/`

```python
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from pydantic import BaseModel, Field

# 1. å­—ç¬¦ä¸²è§£æå™¨
str_parser = StrOutputParser()
chain = model | str_parser
chain.invoke("Hello")  # è¿”å› str è€Œä¸æ˜¯ AIMessage

# 2. JSON è§£æå™¨
json_parser = JsonOutputParser()

prompt = ChatPromptTemplate.from_template(
    "Output a JSON with 'name' and 'age' for: {person}"
)
chain = prompt | model | json_parser
chain.invoke({"person": "a 30-year-old named Alice"})
# â†’ {"name": "Alice", "age": 30}

# 3. Pydantic è§£æå™¨ï¼ˆç±»å‹å®‰å…¨ï¼‰
class Person(BaseModel):
    name: str = Field(description="Person's name")
    age: int = Field(description="Person's age")

model_with_structure = model.with_structured_output(Person)
result = model_with_structure.invoke("Tell me about a 30-year-old named Bob")
# â†’ Person(name="Bob", age=30)
```

## ğŸ§  æœ€ç»ˆç»¼åˆæŒ‘æˆ˜

è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ LangChain åº”ç”¨ï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š

1. **éœ€æ±‚ï¼š** æŠ€æœ¯æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
2. **åŠŸèƒ½ï¼š**
   - ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ï¼ˆPDF/TXTï¼‰
   - ç³»ç»Ÿå»ºç«‹å‘é‡ç´¢å¼•
   - ç”¨æˆ·æé—®ï¼Œç³»ç»ŸåŸºäºæ–‡æ¡£å›ç­”
   - å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè°ƒç”¨æœç´¢å·¥å…·
   - ä¿å­˜å¯¹è¯å†å²

**æç¤ºï¼š** ä½ éœ€è¦ç”¨åˆ°ï¼š
- `ChatPromptTemplate`
- `VectorStore` + `Retriever`
- `Tools` (æœç´¢å·¥å…·)
- `Agent` æˆ– `RunnableWithMessageHistory`
- `Callbacks` (å¯é€‰ï¼Œç”¨äºè¿½è¸ª)

**æ€è€ƒï¼š** ä½ ä¼šå¦‚ä½•è®¾è®¡è¿™ä¸ªç³»ç»Ÿçš„æ¶æ„ï¼Ÿç”»å‡ºæ•°æ®æµå›¾ã€‚

---

## ğŸ“ å­¦ä¹ å®Œæˆï¼

æ­å–œä½ å®Œæˆäº†æ‰€æœ‰ 8 ä¸ªæ¨¡å—ï¼ç°åœ¨ä½ åº”è¯¥ï¼š

âœ… ç†è§£ LangChain çš„æ¶æ„å’Œè®¾è®¡å“²å­¦
âœ… æŒæ¡ Messagesã€Promptsã€Runnables çš„ä½¿ç”¨
âœ… èƒ½å¤Ÿä½¿ç”¨ LCEL æ„å»ºå¤æ‚çš„é“¾
âœ… ç†è§£å·¥å…·å’Œ Agent çš„å·¥ä½œåŸç†
âœ… çŸ¥é“å¦‚ä½•åº”ç”¨é«˜çº§ç‰¹æ€§ï¼ˆStreamingã€Memoryã€RAGï¼‰

**ä¸‹ä¸€æ­¥ï¼š**
1. é˜…è¯»å®˜æ–¹æ–‡æ¡£æ·±å…¥ç‰¹å®šä¸»é¢˜
2. æŸ¥çœ‹ `libs/core/tests/` ä¸­çš„æµ‹è¯•ç”¨ä¾‹å­¦ä¹ æœ€ä½³å®è·µ
3. æ„å»ºå®é™…é¡¹ç›®å·©å›ºçŸ¥è¯†

**æ¨èç»ƒä¹ é¡¹ç›®ï¼š**
- ä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
- å¤šæ¨¡æ€èŠå¤©æœºå™¨äººï¼ˆæ”¯æŒå›¾ç‰‡ï¼‰
- è‡ªåŠ¨åŒ–ç ”ç©¶åŠ©æ‰‹ï¼ˆå¸¦æœç´¢å’Œæ€»ç»“ï¼‰
- ä»£ç åˆ†æå·¥å…·ï¼ˆä½¿ç”¨ AST å’Œ LLMï¼‰

ç¥ä½ åœ¨ LangChain å¼€å‘ä¹‹æ—…ä¸­å–å¾—æˆåŠŸï¼ğŸš€
